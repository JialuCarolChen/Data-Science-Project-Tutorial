{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Querying and Summarising with SQL \n",
    "\n",
    "This week is about working with an existing database.\n",
    "\n",
    "The following tutorial assumes a bit of background on SQL, in particular on its core commands \n",
    "to create new tables and to retrieve data:\n",
    "\n",
    " SQL Command   |  Meaning\n",
    " --------------|------------\n",
    " SELECT COUNT(\\*) FROM *T*   | count how many tuples are stored in table *T*\n",
    " SELECT \\* FROM *T*          | list the content of table *T*\n",
    " SELECT \\* FROM *T* LIMIT *n* | only list  *n* tuples from a table\n",
    " SELECT \\* FROM *T* ORDER BY *a* | order the result by attribute *a* (in ascending order; add DESC for descending order)\n",
    "\n",
    "You can learn more background on these SQL commands in the [Python&SQL tutorial part in Grok][1] (Section 16 onwards).\n",
    "\n",
    "  [1]: https://groklearning.com/course/usyd-comp5310-2016-s1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 1: Data Loading and Exploring for Astronomy Database\n",
    "\n",
    "### Step1: Loading Example Data\n",
    "\n",
    "The first step is to make sure that the example data set is fully loaded into our PostgreSQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't solved last weeks tutorial yet, we have prepared an SQL data dump which you can directly load into your own database.\n",
    "\n",
    "First you need to upload the corresponding data file into your Jupyter instance.\n",
    "Please go to the Resources page of Piazza and download the file **astronomy_db.sql**.\n",
    "\n",
    "Then upload **astronomy_db.sql** into your own Jupyter file space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, open a Terminal window:\n",
    "\n",
    "![New Terminal](http://www.it.usyd.edu.au/~roehm/teaching/comp5310/screenshot_postgres-terminal-new.png \"New Terminal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: Backup your schema first\n",
    "\n",
    "Because we will overwrite certain tables in your database in the subsequent step, you may want to backup your data first if you have already worked on PostgreSQL the previous week.\n",
    "\n",
    "The command to backup (dump) your PostgreSQL database is **<tt>COPY</tt>**.\n",
    "\n",
    "At the <u>terminal prompt</u>, enter the following (**replace LOGINNAME with your Jupyter login name**):\n",
    "\n",
    "<pre>\n",
    "pg_dump LOGINNAME >backupdump.sql\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Astronomy DB\n",
    "After you have secured a backup of your current database, we can continue loading the new astronomy data set. \n",
    "Type in the following command:\n",
    "<pre>psql -f astronomy_db.sql</pre>\n",
    "\n",
    "This should load the content of the dump file into your own database.\n",
    "You can check this afterwords by running **psql** and executing its **<tt>\\d</tt>** command:\n",
    "\n",
    "    psql\n",
    "    \\d\n",
    "    \\d *tablename*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep working with **psql** for the moment.\n",
    "\n",
    "Let's have a look around of the data set which we loaded.\n",
    "\n",
    "<pre>\n",
    "  SELECT COUNT(*) FROM FrequencyBand;\n",
    "  SELECT * FROM FrequencyBand;\n",
    "  SELECT frequency1 FROM FrequencyBand WHERE band=5;\n",
    "\n",
    "  SELECT COUNT(*) FROM Epoch;\n",
    "  SELECT * FROM Epoch;\n",
    "\n",
    "  SELECT COUNT(*) FROM Galaxy;\n",
    "  SELECT * FROM Galaxy LIMIT 5;\n",
    "</pre>\n",
    "\n",
    "Using those command patterns, feel free to explore the database a bit further yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP PLEASE. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 2: Querying Data with SQL\n",
    "\n",
    "After we loaded and initially explored the data set, we continue using SQL queries a bit more.\n",
    "\n",
    "We still remain working with **psql** in the Terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL: Joins\n",
    "\n",
    "If you need to combine data from multiple tables, you can **join** those as follows.\n",
    "\n",
    "**Example:** We would like to find out the details on when and how the radio telescope was used to observe galaxies in the 20 GHz frequency band.\n",
    "\n",
    "We first can have a look at the <tt>Epoch</tt> table:\n",
    "<pre>\n",
    "SELECT epochid, config, startdate, enddate \n",
    "  FROM Epoch\n",
    " WHERE band = 20; \n",
    "</pre>\n",
    "\n",
    "If you execute the SQL query above, you see that the 20 GHz band was measured at seven different epochs over the course of five years. But we cannot see the telescope configuration details directly, just an internal ID the refers to the <tt>TelescopeConfig</tt> table.\n",
    "\n",
    "You could now look into that table too with a second query and check for the seven configurations mentioned above - but that is tedious and error prone...\n",
    "<pre>\n",
    "SELECT * FROM TelescopeConfig;\n",
    "</pre>\n",
    "\n",
    "The correct way is to use this *foreign key* attribute <tt>config</tt> from the <tt>Epoch</tt> table to **join** both the <tt>Epoch</tt> and the <tt>TelescopeConfig</tt> tables and retreive the macthing values in just one query:\n",
    "\n",
    "<pre>\n",
    "SELECT epochid, startdate, enddate, mindec, maxdec, tele_array, baseline\n",
    "  FROM Epoch JOIN TelescopeConfig ON (config = configId)\n",
    " WHERE band = 20;\n",
    "</pre>\n",
    "\n",
    "**Note:** You could have used <tt>SELECT *</tt> above too, but then the result would become too large, so that psql would have started its pager tool. If this happens to you, you can scroll with the cursors or the space bar, and leave the pager tool by pressing 'q'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with DATE values \n",
    "\n",
    "For most data types in SQL - notably integers, strings, floating point numbers - the standard comparison and numerical operations apply.\n",
    "\n",
    "The handling of <tt>DATE</tt> is a bit delicate though. You can compare them using date strings, but the standard date format can be configured differently in database systems than you expect (eg. 'yyyy-mm-dd' vs. 'mm/dd/yyyy' etc), so that these kind of codes are difficult to port.\n",
    "\n",
    "<pre>\n",
    "SELECT *\n",
    "  FROM Epoch\n",
    " WHERE startdate = '2006-04-29';\n",
    "SELECT *\n",
    "  FROM Epoch\n",
    " WHERE startdate = '29/04/2006'; \n",
    "</pre>\n",
    "\n",
    "The SQL **EXTRACT()** function provides a convenient way to access any part of a date value. For example, **extract(year from datevar)** allows to extract the year component of a given date cariable *datevar*.\n",
    "For a full description of all components available to *extract()*, see [the PostgreSQL online documentation][1].\n",
    "\n",
    "**Example:**\n",
    "<pre>\n",
    "SELECT *\n",
    "  FROM Epoch\n",
    " WHERE extract(year from startdate) = 2006;\n",
    "</pre>\n",
    "\n",
    " [1]:[http://www.postgresql.org/docs/current/static/functions-datetime.html#FUNCTIONS-DATETIME-EXTRACT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL: Aggregation Functions\n",
    "\n",
    "SQL supports multiple aggregation functions.\n",
    "\n",
    " SQL Aggregate Function | Meaning\n",
    " --- | ---\n",
    " COUNT(\\*)   | count all tuples in a table\n",
    " COUNT(attr) | count the tuples with a non-NULL value in attr\n",
    " MIN(attr)   | determine the minimum value of attr (ignores NULL)\n",
    " MAX(attr)   | determine the maximum value of attr (ignores NULL)\n",
    " AVG(attr)   | determine the average value of numeric attr (arithmetic mean) (ignores NULL)\n",
    " SUM(attr)   | calculates the sum of a numeric attr (ignores NULL)\n",
    "\n",
    "\n",
    "\n",
    "Try some out:\n",
    "\n",
    "\n",
    "**Question:** In which range (minimum to maximum declanation) did the telescope do the measurements?\n",
    "<pre>\n",
    "SELECT MIN(mindec), MAX(maxdec) FROM TelescopeConfig; \n",
    "</pre>\n",
    "\n",
    "**Question:** In which range (minimum to maximum declanation) did the telescope do specifically the 20 GHz band measurements?\n",
    "<pre>\n",
    "SELECT MIN(mindec), MAX(maxdec)\n",
    "  FROM Epoch JOIN TelescopeConfig ON (config = configId)\n",
    " WHERE band = 20;\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Statistical Aggregates\n",
    "\n",
    "SQL also supports some statistical aggregates. The syntax is a bit more complex, as they work on ordered sets. This order has to be first specified with an *WITHIN GROUP* clause in SQL so that aggregates like 'Median' or 'Percentile' make sense.\n",
    "\n",
    "Statistics Aggregate | Meaning\n",
    "---|---\n",
    "MODE()  WITHIN GROUP (ORDER BY *attr*) |  mode function over *attr*\n",
    "PERCENTILE_DISC(0.5) WITHIN GROUP (ORDER BY *attr*) | median of the *attr* values\n",
    "PERCENTILE_DISC(*p*) WITHIN GROUP (ORDER BY *attr*) | *p* percentile of the *attr* values\n",
    "\n",
    "**Example:** Statistical analysis over the intensity values of *all* measurements.\n",
    "\n",
    "<pre>\n",
    "SELECT COUNT(intensity),\n",
    "       MIN(intensity),\n",
    "       Max(intensity), \n",
    "       MAX(intensity) - MIN(intensity)                           AS Range, \n",
    "       AVG(intensity)                                            AS Mean,\n",
    "       MODE()  WITHIN GROUP (ORDER BY intensity)                 AS Mode, \n",
    "       PERCENTILE_DISC(0.5) WITHIN GROUP (ORDER BY intensity)    AS Median,\n",
    "       PERCENTILE_DISC(0.25) WITHIN GROUP (ORDER BY intensity)   AS Percentile25, \n",
    "       PERCENTILE_DISC(0.75) WITHIN GROUP (ORDER BY intensity)   AS Percentile75,\n",
    "       PERCENTILE_DISC(0.75) WITHIN GROUP (ORDER BY intensity)\n",
    "       - PERCENTILE_DISC(0.25) WITHIN GROUP (ORDER BY intensity) AS IQR \n",
    "  FROM Measurement;\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOUR TASK:\n",
    "\n",
    "Answer the following questions with an SQL query:\n",
    "\n",
    "1.  In which time period were all the measurement done?\n",
    "\n",
    "2.  At how many distinct frequencies were measured? Which frequencies?\n",
    "\n",
    "3.  Do the same statistical analysis for measurements as above, but for just measurements from the year 2004;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE SOLUTION:\n",
    "\n",
    "1.  In which time period were all the measurement done?\n",
    "   <pre>\n",
    "   SELECT MIN(startdate), MAX(enddate) FROM Epoch;\n",
    "   </pre>\n",
    "\n",
    "2.  At how many distinct frequencies were measured? Which frequencies?\n",
    "\n",
    "    *You could think answering this query with a SELECT COUNT(frequency1)+COUNT(frequency2) FROm FrequencyBand, but then this approach would double count any frequency occuring as both frequency1 and frequency2. Hence the following approach with a sub-query is needed which combines both frequency values into one intermediate result which then gets counted.*\n",
    "<pre>\n",
    "SELECT COUNT(DISTINCT freq)\n",
    "  FROM ( SELECT frequency1 AS freq\n",
    "           FROM FrequencyBand\n",
    "         UNION\n",
    "         SELECT frequency2 AS freq\n",
    "           FROM FrequencyBand ) AS AllFrequencies;\n",
    "   </pre>\n",
    "\n",
    "3.  Do the same statistical analysis for measurements as above, but for just measurements from the year 2004;\n",
    "\n",
    "   *This query needs a join in order to get access to the start date of each measurement, as well as an extract() function to determine the year of each measurement from startdate.*\n",
    "   <pre>\n",
    "SELECT COUNT(intensity),\n",
    "       MIN(intensity),\n",
    "       Max(intensity), \n",
    "       MAX(intensity) - MIN(intensity)                           AS Range, \n",
    "       AVG(intensity)                                            AS Mean,\n",
    "       MODE()  WITHIN GROUP (ORDER BY intensity)                 AS Mode, \n",
    "       PERCENTILE_DISC(0.5) WITHIN GROUP (ORDER BY intensity)    AS Median,\n",
    "       PERCENTILE_DISC(0.25) WITHIN GROUP (ORDER BY intensity)   AS Percentile25, \n",
    "       PERCENTILE_DISC(0.75) WITHIN GROUP (ORDER BY intensity)   AS Percentile75,\n",
    "       PERCENTILE_DISC(0.75) WITHIN GROUP (ORDER BY intensity)\n",
    "       - PERCENTILE_DISC(0.25) WITHIN GROUP (ORDER BY intensity) AS IQR \n",
    "  FROM Measurement M JOIN Epoch E ON (epoch=epochId AND M.band=E.band)\n",
    " WHERE extract(year from startdate) = 2004;\n",
    " </pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP PLEASE. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 3: Data Gathering from an SQL Database\n",
    "\n",
    "In this next exercise, we will be looking into how to retrieve data from an existing SQL database into a Python program for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: DB Connection and Query Execution\n",
    "\n",
    "In the first step, we are repeating the basic database connection phase from the tutorial in Week 4 and we execute a simple SQL query on that database.\n",
    "\n",
    "Note that you can use this code fragment to also execute any SQL statement which we otherwise discuss as part of this tutorial without the need to go to the Jupyter/psql terminal screen. In case that you browser does not support copy/paste for the Jupyter terminal, this might be the faster way to work in this SQL tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATABASENAME = 'LOGINNAME'  # please replace with your own Jupyter login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "def pgconnect():\n",
    "    try: \n",
    "        conn = psycopg2.connect(database=DATABASENAME)\n",
    "        print('connected')\n",
    "    except Exception as e:\n",
    "        print(\"unable to connect to the database\")\n",
    "        print(e)\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import psycopg2.extras\n",
    "\n",
    "def pgquery( conn, sqlcmd, args, silent=False, returntype='tuple'):\n",
    "   \"\"\" utility function to execute some SQL query statement\n",
    "       it can take optional arguments (as a dictionary) to fill in for placeholder in the SQL\n",
    "       will return the complete query result as return value - or in case of error: None\n",
    "       error and transaction handling built-in (by using the 'with' clauses) \"\"\"\n",
    "   retval = None\n",
    "   with conn:\n",
    "      cursortype = None if returntype != 'dict' else psycopg2.extras.RealDictCursor\n",
    "      with conn.cursor(cursor_factory=cursortype) as cur:\n",
    "         try:\n",
    "            if args is None:\n",
    "                cur.execute(sqlcmd)\n",
    "            else:\n",
    "                cur.execute(sqlcmd, args)\n",
    "            retval = cur.fetchall() # we use fetchall() as we expect only _small_ query results\n",
    "         except Exception as e:\n",
    "            if e.pgcode != None and not(silent):\n",
    "                print(\"db read error: \")\n",
    "                print(e)\n",
    "   return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# connect to your database\n",
    "conn = pgconnect()\n",
    "    \n",
    "# prepare SQL statement\n",
    "query_stmt = \"SELECT * FROM FrequencyBand\"\n",
    "\n",
    "# execute query and print result\n",
    "query_result = pgquery (conn, query_stmt, None)\n",
    "print(query_stmt)\n",
    "print(query_result)\n",
    "\n",
    "# prepare another SQL statement including placeholders\n",
    "query_stmt = \"SELECT * FROM FrequencyBand WHERE band=%(band)s\"\n",
    "\n",
    "# define the 'band' parameter, execute query+parameters. and print result\n",
    "param = {'band' : 20}\n",
    "#query_result = pgquery (conn, query_stmt, param)\n",
    "print(query_stmt)\n",
    "print(query_result)\n",
    "\n",
    "# cleanup\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course you do not need to just print the result of a database operation directly to the screen. Once it is in a variable in your Python program, you can work with it as with any other data which you have loaded, eg. from a CSV file before.\n",
    "\n",
    "**Note** that the data read from the postgresql database differs in its typing from the data we retrieved from CSV files so far using the CSV.DictReader:\n",
    " - SQL returns by default a **list of tuples**, while the data read with the CSV reader is a **list of dictionaries**.\n",
    " - The attributes in the tuples of the SQL result are **typed** according to the SQL schema, while the CSV data is **always strings** and hence needs to be type-converted first.\n",
    " \n",
    "The differences is the addressability of each component - in one case positionally, in the other as key-value pairs, and whether we need further type conversions from strings to numbers, or not. \n",
    "\n",
    "The following code snippet demonstrates these typing differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# here the type and content analysed for the SQL query result from above\n",
    "print(\"Analysis of the SQL result types - first whole result, then just first entry:\")\n",
    "print( type(query_result) )\n",
    "print( query_result )\n",
    "print( type(query_result[0]) )\n",
    "print( query_result[0] )\n",
    "print( type(query_result[0][0]) )\n",
    "print( query_result[0][0] )\n",
    "\n",
    "# and now for comaprison the type and values read from the raw CSV file\n",
    "import csv\n",
    "data_frequencies = list(csv.DictReader(open('04-at20g-short-frequencies.csv')))\n",
    "\n",
    "print(\"Analysis of the CSV result types - first whole result, then just first entry:\")\n",
    "print( type(data_frequencies) )\n",
    "print(data_frequencies)\n",
    "print( type(data_frequencies[0]) )\n",
    "print(data_frequencies[0])\n",
    "print( type(data_frequencies[0]['abbreviated_frequency_ghz']) )\n",
    "print(data_frequencies[0]['abbreviated_frequency_ghz']) # we need to know the attribute key\n",
    "print(data_frequencies[0][0]) # does not work!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read data from a database also into a dictionary, where the keys of each value will be the attribute names from the database schema. This needs a special kind of SQL cursor, a so-called dictionary cursor, which uses the attribute names from the database schema as column keys. The previusly introduced *pgquery()* function allows to pass a 'returntype' argument with which we can control its return type. It controls just a small code variation in how the query cursor is opened. If you set this parameter value to 'dict', we will get the query result as a Python dictionary (dict) returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# connect to your database\n",
    "conn = pgconnect()\n",
    "    \n",
    "# prepare SQL statement\n",
    "query_stmt = \"\"\"SELECT *\n",
    "                  FROM FrequencyBand\"\"\"\n",
    "\n",
    "# execute query and print result\n",
    "query_result = pgquery (conn, query_stmt, None, returntype='dict')\n",
    "print(query_result)\n",
    "\n",
    "# cleanup\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data Visualisation of Query Results\n",
    "\n",
    "Next we want to do some data visualisation with data read from a SQL database.\n",
    "\n",
    "The **make_plot()** function below will take any query result and turn it into either a simple bar chart, or a scatter plot. Which one you can control with the last 'categorica' argument which schould be True for a bar chart, otherwise false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def make_plot(data, x_key, y_key, title, xlabel=None, ylabel=None, bar_width=0.5, categorical=True):\n",
    "    xlabel = xlabel or x_key\n",
    "    ylabel = ylabel or y_key\n",
    "    xs = [row[x_key] for row in data]\n",
    "    ys = [row[y_key] for row in data]\n",
    "    \n",
    "    if categorical:\n",
    "        plt.bar(range(len(data)), ys, width=bar_width)\n",
    "        plt.xticks(np.arange(len(data))+bar_width/2., xs)\n",
    "    else:\n",
    "        plt.scatter(xs, ys)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use this function to plot our previous query result in first a bar chart, and then a scatter plot of the 'band' value versus the 'frequency1' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for r in query_result:\n",
    "    print(r)\n",
    "    \n",
    "make_plot(\n",
    "    query_result,\n",
    "    x_key='band',\n",
    "    y_key='frequency1',\n",
    "    title='Frequency Bands',\n",
    "    categorical=True)\n",
    "\n",
    "make_plot(\n",
    "    query_result,\n",
    "    x_key='band',\n",
    "    y_key='frequency1',\n",
    "    title='Frequency Bands',\n",
    "    categorical=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The code above assumes that you have the query_result from the previous query in a *dict()* type. However the **make_plot()** function would work with a list of tuples too. In this case, simply provide the positional values of the x- and y-attributes for *x_key* and *y_key* (like for example 0 and 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOUR TASK:\n",
    "\n",
    "Next visualise something more interesting, for example visualise the result of the following sql query:\n",
    "<pre>\n",
    "SELECT epoch, COUNT(DISTINCT gid)\n",
    "  FROM Measurement\n",
    " GROUP BY epoch\n",
    " ORDER BY epoch;\n",
    "</pre>\n",
    "\n",
    "\n",
    "Try out some other code examples from Week 3 that visualises the data read from the SQL database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXAMPLE SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# connect to your database\n",
    "conn = pgconnect()\n",
    "    \n",
    "# prepare SQL statement\n",
    "query_stmt = \"\"\"SELECT epoch, COUNT(DISTINCT gid)\n",
    "                  FROM Measurement\n",
    "                 GROUP BY epoch\n",
    "                 ORDER BY epoch;\"\"\"\n",
    "\n",
    "# execute query and print result\n",
    "query_result = pgquery (conn, query_stmt, None, returntype='dict')\n",
    "print(query_result)\n",
    "\n",
    "#visualise\n",
    "make_plot(\n",
    "    query_result,\n",
    "    x_key='epoch',\n",
    "    y_key='count',\n",
    "    title='Distinct Galaxies observed per Epoch',\n",
    "    categorical=True)\n",
    "\n",
    "make_plot(\n",
    "    query_result,\n",
    "    x_key='epoch',\n",
    "    y_key='count',\n",
    "    title='Distinct Galaxies observed per Epoch',\n",
    "    categorical=False)\n",
    "\n",
    "# cleanup\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising the galaxy location distribution\n",
    "\n",
    "We first need two utility functions which convert the stored RA and DEC values to normalised radiants.\n",
    "In python, there is the specific **astLib** library, which we however haven't installed on our jupyter server. We hence introduce conversion functions on the SQL level.\n",
    "\n",
    "The following calculation follow [http://www.projectrho.com/public_html/starmaps/trigonometry.php]\n",
    "\n",
    "RA is the *Right Ascension* which is stored in our data set as string in terms of hours, minutes and seconds. \n",
    "To convert to cartesian coordinates:\n",
    "<pre>\n",
    "CREATE OR REPLACE FUNCTION ra2phi ( ra VARCHAR ) RETURNS FLOAT AS\n",
    "$$  SELECT  CAST(split_part(ra, ':', 1) AS FLOAT) * 15\n",
    "          + CAST(split_part(ra, ':', 2) AS FLOAT) * 0.25\n",
    "          + CAST(split_part(ra, ':', 3) AS FLOAT) * 0.0041666 $$\n",
    "LANGUAGE SQL;\n",
    "</pre>\n",
    "\n",
    "DEC is the *declination* (think latitude). It is stored as string of degrees minutes and seconds. It goes from +90 (north pole) to -90 degrees (south pole). \n",
    "To convert to cartesian coordinates:\n",
    "<pre>\n",
    "CREATE OR REPLACE FUNCTION dec2theta ( dc VARCHAR ) RETURNS FLOAT AS\n",
    "$$  SELECT (  ABS(CAST(split_part(dc, ':', 1) AS FLOAT))\n",
    "           + CAST(split_part(dc, ':', 2) AS FLOAT) / 60\n",
    "           + CAST(split_part(dc, ':', 3) AS FLOAT) / 3600 ) * SIGN(CAST(split_part(dc, ':', 1) AS INT)) $$\n",
    "LANGUAGE SQL;\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# connect to your database\n",
    "conn = pgconnect()\n",
    "    \n",
    "# prepare SQL statement\n",
    "query_stmt = \"\"\"SELECT ra2phi(ra) AS phi, dec2theta(dec) AS theta FROM Galaxy;\"\"\"\n",
    "\n",
    "# execute query and print result\n",
    "query_result = pgquery (conn, query_stmt, None, returntype='dict')\n",
    "\n",
    "#visualise\n",
    "make_plot(\n",
    "    query_result,\n",
    "    x_key='phi',\n",
    "    y_key='theta',\n",
    "    title='Coordinates of observed Galaxies',\n",
    "    categorical=False)\n",
    "\n",
    "# prepare SQL statement\n",
    "query_stmt = \"\"\"SELECT ra2phi(ra) AS phi, dec2theta(dec) AS theta FROM Galaxy WHERE ra2phi(ra) < 100;\"\"\"\n",
    "\n",
    "# execute query and print result\n",
    "query_result = pgquery (conn, query_stmt, None, returntype='dict')\n",
    "\n",
    "#visualise\n",
    "make_plot(\n",
    "    query_result,\n",
    "    x_key='phi',\n",
    "    y_key='theta',\n",
    "    title='Coordinates of Galaxies without Outliers',\n",
    "    categorical=False)\n",
    "\n",
    "# cleanup\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert this further to **x** and **y** coordinates, still following [http://www.projectrho.com/public_html/starmaps/trigonometry.php]:\n",
    "\n",
    "RVECT = DISTANCE * COS[ THETA ]\n",
    "\n",
    "X = RVECT * COS[ PHI ]\n",
    "\n",
    "Y = RVECT * SIN[ PHI ]\n",
    "\n",
    "We assume DISTANCE to be 10 for the purpose of the following calculation as we have no distance value in our data set.\n",
    "<pre>\n",
    "CREATE OR REPLACE FUNCTION radec2x ( ra VARCHAR, dc VARCHAR ) RETURNS FLOAT AS\n",
    "$$  SELECT  10 * cos(dec2theta(dc)) * cos(ra2phi(ra)) $$\n",
    "LANGUAGE SQL;\n",
    "CREATE OR REPLACE FUNCTION radec2y ( ra VARCHAR, dc VARCHAR ) RETURNS FLOAT AS\n",
    "$$  SELECT  10 * cos(dec2theta(dc)) * sin(ra2phi(ra)) $$\n",
    "LANGUAGE SQL;\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# connect to your database\n",
    "conn = pgconnect()\n",
    "    \n",
    "# prepare SQL statement\n",
    "query_stmt = \"\"\"SELECT radec2x(ra,dec) AS x, radec2y(ra,dec) AS y FROM Galaxy WHERE ra2phi(ra) < 100;\"\"\"\n",
    "\n",
    "# execute query and print result\n",
    "query_result = pgquery (conn, query_stmt, None, returntype='dict')\n",
    "\n",
    "#visualise\n",
    "make_plot(\n",
    "    query_result,\n",
    "    x_key='x',\n",
    "    y_key='y',\n",
    "    title='Coordinates of Galaxies without Outliers',\n",
    "    categorical=False)\n",
    "\n",
    "# cleanup\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP PLEASE. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 4: Summarising Data with SQL\n",
    "\n",
    "In the next exercise, we look at the SQL language in a bit more depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL: Data Analysis with GROUP BY\n",
    "\n",
    "So far, our aggregate functions were always applied to all tuples in a table.\n",
    "Sometimes it is however very useful to group  rows into distinct partitions and then aggregate for each partition separatly. This is what the **GROUP BY** clause of SQL is doing.\n",
    "\n",
    "**Example 1:**\n",
    "How many measurements were done *per each galaxy*?\n",
    "<pre>\n",
    "  SELECT gid, COUNT(*)\n",
    "    FROM Measurement\n",
    "   GROUP BY gid;\n",
    "</pre>\n",
    "\n",
    "**Example 2:**\n",
    "How many measurements of *distinct* galaxies were done *per each epoch*?\n",
    "<pre>\n",
    "SELECT epoch, COUNT(DISTINCT gid)\n",
    "  FROM Measurement\n",
    " GROUP BY epoch\n",
    " ORDER BY epoch;\n",
    "</pre>\n",
    "\n",
    "**Example 3:**\n",
    "Determine some basic statistics about the measured intensity values *per each ferquency band*, including minimum intensity, maximum intensity, range of intensity values, mean, mode, 25th and 75th percentile:\n",
    "<pre>\n",
    "SELECT M.band,\n",
    "       MIN(intensity), \n",
    "       Max(intensity), \n",
    "       MAX(intensity) - MIN(intensity)                           AS Range,\n",
    "       AVG(intensity)                                            AS Mean,\n",
    "       MODE()  WITHIN GROUP (ORDER BY intensity)                 AS Mode, \n",
    "       PERCENTILE_DISC(0.5) WITHIN GROUP (ORDER BY intensity)    AS Median,\n",
    "       PERCENTILE_DISC(0.25) WITHIN GROUP (ORDER BY intensity)   AS Percentile25,\n",
    "       PERCENTILE_DISC(0.75) WITHIN GROUP (ORDER BY intensity)   AS Percentile75,\n",
    "       PERCENTILE_DISC(0.75) WITHIN GROUP (ORDER BY intensity)\n",
    "       - PERCENTILE_DISC(0.25) WITHIN GROUP (ORDER BY intensity) AS IQR \n",
    "  FROM Measurement M JOIN Epoch E (epoch=epochid)\n",
    "  WHERE extract(year from startdate) = 2006\n",
    " GROUP BY M.band;\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOUR TASK:\n",
    "\n",
    "Answer the following questions with SQL GROUP-BY queries:\n",
    "\n",
    "1. Determine the same per-band statistics as in the last grouping query just for measurements in 2006.\n",
    "\n",
    "2. Same than in (1), but just those bands with at least 300 measurements.\n",
    "\n",
    "3. How many observation were done in the 20 GHz, 8 GHz and 5 GHz bands in 2006?\n",
    "\n",
    "4. In which epoch were the most measurements done?\n",
    "\n",
    "5. List all observations which were done in all three bands and where the polarized intensity (flux) of at least one band was 50 mJy or higher.\n",
    "\n",
    "6. Which sources were observed multiple times (in different epochs)?\n",
    "For each re-observed source, show per frequency band its average flux and the variability of their intensity ((max-min)/max) over all epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXAMPLE SOLUTION\n",
    "\n",
    "1. Determine the same per-band statistics as in the last grouping query just for measurements in 2006.\n",
    "<pre>\n",
    "SELECT M.band,                                                                                        \n",
    "       MIN(intensity),                                                                                      \n",
    "       Max(intensity),                                                                                      \n",
    "       MAX(intensity) - MIN(intensity)                           AS Range,                                  \n",
    "       AVG(intensity)                                            AS Mean,                                   \n",
    "       MODE()  WITHIN GROUP (ORDER BY intensity)                 AS Mode,                                   \n",
    "       PERCENTILE_DISC(0.5) WITHIN GROUP (ORDER BY intensity)    AS Median,                                 \n",
    "       PERCENTILE_DISC(0.25) WITHIN GROUP (ORDER BY intensity)   AS Percentile25,                           \n",
    "       PERCENTILE_DISC(0.75) WITHIN GROUP (ORDER BY intensity)   AS Percentile75,                           \n",
    "       PERCENTILE_DISC(0.75) WITHIN GROUP (ORDER BY intensity)                                              \n",
    "       - PERCENTILE_DISC(0.25) WITHIN GROUP (ORDER BY intensity) AS IQR                                     \n",
    "  FROM Measurement M JOIN Epoch E ON (epoch=epochId AND M.band=E.band) \n",
    " WHERE extract(year from startdate) = 2006\n",
    " GROUP BY M.band;</pre>\n",
    " \n",
    "2. Same than in (1), but just those bands with at least 300 measurements.\n",
    "<pre>\n",
    "SELECT M.band,\n",
    "       COUNT(*),                                                                                        \n",
    "       MIN(intensity),                                                                                      \n",
    "       Max(intensity),                                                                                      \n",
    "       MAX(intensity) - MIN(intensity)                           AS Range,                                  \n",
    "       AVG(intensity)                                            AS Mean,                                   \n",
    "       MODE()  WITHIN GROUP (ORDER BY intensity)                 AS Mode,                                   \n",
    "       PERCENTILE_DISC(0.5) WITHIN GROUP (ORDER BY intensity)    AS Median,                                 \n",
    "       PERCENTILE_DISC(0.25) WITHIN GROUP (ORDER BY intensity)   AS Percentile25,                           \n",
    "       PERCENTILE_DISC(0.75) WITHIN GROUP (ORDER BY intensity)   AS Percentile75,                           \n",
    "       PERCENTILE_DISC(0.75) WITHIN GROUP (ORDER BY intensity)                                              \n",
    "       - PERCENTILE_DISC(0.25) WITHIN GROUP (ORDER BY intensity) AS IQR                                     \n",
    "  FROM Measurement M JOIN Epoch E ON (epoch=epochId AND M.band=E.band) \n",
    " WHERE extract(year from startdate) = 2006\n",
    " GROUP BY M.band\n",
    "HAVING COUNT(*) >= 300;</pre>\n",
    "\n",
    "3. How many observation were done in the 20 GHz, 8 GHz and 5 GHz bands in 2006?\n",
    "<pre>\n",
    "SELECT band, COUNT(*) \n",
    "  FROM Measurement NATURAL JOIN Epoch \n",
    " WHERE Extract(year from startDate) = 2006 OR Extract(year from endDate) = 2006\n",
    " GROUP BY band;</pre>\n",
    "\n",
    "4. In which epoch were the most measurements done?\n",
    "<pre>\n",
    "SELECT epoch, COUNT(*) AS cnt\n",
    "  FROM Measurement\n",
    " GROUP BY epoch\n",
    " ORDER BY cnt DESC\n",
    " LIMIT 1;</pre>\n",
    " \n",
    "5. List all observations which were done in all three bands and where the polarized intensity (flux) of at least one band was 50 mJy or higher.\n",
    "<pre>\n",
    "SELECT * \n",
    "  FROM Measurement M1\n",
    " WHERE  (SUBSTR(polarisation,1,1)!= '&lt;' AND polarisation>=50)\n",
    "   AND 3 = ( SELECT COUNT(*)\n",
    "              FROM Measurement M2 \n",
    "             WHERE M2.gid=M1.gid AND M2.epochID=M1.epochID)</pre>\n",
    "             \n",
    "6. Which sources were observed multiple times (in different epochs)?\n",
    "<pre>\n",
    "SELECT gid, band, AVG(intensity), (MAX(intensity)-MIN(intensity))/MAX(intensity)\n",
    "  FROM Variability \n",
    "GROUP BY gid, band\n",
    "HAVING COUNT(*) >= 2;</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP PLEASE. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Data Gathering from the Web\n",
    "In this last exercise, we will be looking into how to use Python to collect data from a web service using a JSON API (application programming interface)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example is adapted from the corresponding example of Chapter 9 of the \"Data Science from Scratch\" book.\n",
    "We first connect to the  github.com  API and look at the repository of   postgresql:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import  requests, json\n",
    "endpoint= \"https://api.github.com/users/postgres/repos\"\n",
    "repos   = json.loads(requests.get(endpoint).text)\n",
    "print (repos)  # sorry, this will be quite longish to look at ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above's code has already parsed the content of the github response as JSON message.\n",
    "Below's code is now further analysing this JSON object and, for example, determining the months and weekdays of when the last commits were done in the postgresql repository.\n",
    "\n",
    "The also three lines additionally also determine the last five languages used in the PostgreSQL github repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "from collections     import Counter\n",
    "print(len(repos))\n",
    "dates = [parse(repo[\"created_at\"]) for repo in repos]\n",
    "month_counts = Counter(date.month for date in dates)\n",
    "weekday_counts = Counter(date.weekday() for date in dates)\n",
    "print (month_counts)\n",
    "print (weekday_counts)\n",
    "\n",
    "last_5_repositories = sorted(repos,\n",
    "                             key=lambda r: r[\"created_at\"],\n",
    "                             reverse=True)[:5]\n",
    "last_5_languages = [repo[\"language\"]\n",
    "                    for repo in last_5_repositories]\n",
    "print (last_5_languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Data Science from Scratch\" book also contains some example using the Twitter API.\n",
    "We will not have time more here in the lab, but back home you might want to have a look at the corresponding example in Chapter 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing JSON in PostgreSQL\n",
    "\n",
    "The following code is an example of\n",
    " - how to create a table in PostgreSQL including a JSON attribute  (called 'repos' here)\n",
    " - how to insert some JSON data into that table (note the json.dumps() call)\n",
    " - how to query that data back again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# connect to your database\n",
    "conn = pgconnect()\n",
    "    \n",
    "# prepare a JSON-enabled table in PostgreSQL    \n",
    "create_table_stmt = \"\"\"CREATE TABLE IF NOT EXISTS GitHub (\n",
    "                             usr   VARCHAR(20) PRIMARY KEY,\n",
    "                             url   VARCHAR(100),\n",
    "                             repos JSONB\n",
    "                       )\"\"\"\n",
    "pgquery(conn, create_table_stmt, None)\n",
    "\n",
    "insert_stmt = \"\"\"INSERT INTO GitHub VALUES ( %(user)s, %(url)s, %(json)s )\"\"\"\n",
    "param = dict()\n",
    "param['user'] = 'postgres'\n",
    "param['url']  = endpoint\n",
    "param['json'] = json.dumps(repos) # important; need to convert json object to text for insert\n",
    "retval = pgquery(conn, insert_stmt, param)\n",
    "\n",
    "query_stmt = \"SELECT * FROM GitHub\"\n",
    "retval = pgquery(conn, query_stmt, None)\n",
    "print(retval)\n",
    "\n",
    "# cleanup\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOUR TASK:\n",
    "\n",
    "Extend the code above to more detailed query the stored JSON data.\n",
    "\n",
    "Try to select the same result than we did in the previous step before in Python.\n",
    "\n",
    "Documentation of PostgreSQL's JSON support [is available here][1]\n",
    "\n",
    "  [1]: www.postgresql.org/docs/curreâ€¦ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's it for today. THANKS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
