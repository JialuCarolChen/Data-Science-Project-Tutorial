{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transformation, cleaning and loading with Python\n",
    "\n",
    "## Astronomy Data Set\n",
    "We are considering an astronomy data set this week from the Australia Telescope 20GHz Survey  (AT20G) from 2008. In Piazza (Resources section), there is an Excel sheet available with an excerpt of the original data set, as well as an readme document which explains the scenario and all the different attributes in the Excel workbook. We also have made available the content of the Excel workbook as a set of five different CSV files. Please upload those CSV files to Jupyter first. \n",
    "\n",
    "**Important:** Make sure that the naming of all the files is as follows:\n",
    " 1. 04-at20g-short-epochs.csv\n",
    " 2. 04-at20g-short-frequencies.csv\n",
    " 3. 04-at20g-short-main-catalogue.csv\n",
    " 4. 04-at20g-short-observations.csv\n",
    " 5. 04-at20g-short-variabilities.csv\n",
    "\n",
    "\n",
    "## EXERCISE 1: DB Creation and Loading Data\n",
    "\n",
    "The following assumes a bit of background on SQL, in particular on its core commands to create new tables and to retrieve data:\n",
    "\n",
    " SQL Command   |  Meaning\n",
    " --------------|------------\n",
    " CREATE TABLE *T* (...)      | creates a new table *T*; list the attributes in brackets in the form  <tt>attribute type</tt>\n",
    " DROP TABLE *T*              | if needed - removes an existing table *T*\n",
    " INSERT INTO *T* VALUES (..) | inserts a new row into table T\n",
    " DELETE FROM *T*             | deletes *all* rows from table *T*\n",
    " SELECT COUNT(\\*) FROM *T*   | count how many tuples are stored in table *T*\n",
    " SELECT \\* FROM *T*          | list the content of table *T*\n",
    "\n",
    "You can learn more background on these SQl commands in the SQL tutorial part in Grok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Creation, Part 1: PostgreSQL\n",
    "First, we  need to create a target table in our PostgreSQL database. Relational databases work 'schema first': We first have to create a schema which defines the layout and typing of the database tables before we can load and query any data in a relational system. \n",
    "\n",
    "Looking at the source data, we assume two integer columns, the first one being unique.\n",
    "\n",
    "The next step we try to **do outside Python in a Jupyter shell** (we will later show how to do it inside Python, but sometimes shell work is faster):\n",
    "\n",
    "Go to the Jupyter start page and open a Terminal in Jupyter using the 'New' menu:\n",
    "\n",
    "![New Terminal](http://www.it.usyd.edu.au/~roehm/teaching/comp5310/screenshot_postgres-terminal-new.png \"New Terminal\")\n",
    "\n",
    "A new Terminal window now open.\n",
    "\n",
    "Here you can directly connect to the postgresql database using the 'psql' command.\n",
    "\n",
    "Enter 'psql':\n",
    "![PSQL prompt](http://www.it.usyd.edu.au/~roehm/teaching/comp5310/screenshot_postgresql_terminal-prompt.png \"PSQL prompt\")\n",
    "\n",
    "\n",
    "Then on the psql prompt, give the following SQL create table statement:\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS Frequency (\n",
    "       frequency INTEGER PRIMARY KEY,\n",
    "        abbrev    SMALLINT\n",
    "    );\n",
    " \n",
    "You can verify whether you created the table correctly with the \\d command:\n",
    "\n",
    "     \\d\n",
    "     \\d Frequency\n",
    "\n",
    "You should see the following:\n",
    "![Frequency Table Schema](http://www.it.usyd.edu.au/~roehm/teaching/comp5310/screenshot_postgresql_table-frequency \"Frequency Table Schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV File Loading, Part 1: Frequency Data \n",
    "Next we want to load data from an external CSV file.\n",
    "We will use psql's **\\copy** command for this.\n",
    "\n",
    "**Prerequisites:** Make sure, that you have uploaded the CSV files and that the filenames are as specified at the top of this notebook.\n",
    "\n",
    "To load data from a CSV file into a relational database, we have to tell the system\n",
    " - into which table to load the data ('Frequency')\n",
    " - which attributes to expect; this is optional, but if you are unsure whether the order in the CSV columns matches the order of attributes in a table, it is best to specify it here. Basically in our example, we specify that we will read 'frequency' and 'abbrev' values from the CSV file in this order.\n",
    " - from which file to load the data; be sure to use **\\copy** rather than just COPY so that you can use a relatibe filename relative to the current directory\n",
    " - which format to expect (CSV) and whether there is a header row that should be ignored (yes, it is - HEADER)\n",
    "\n",
    "So with all this, the final command to load the frequency table is as follows.\n",
    "Please type into the psql prompt at the terminal:\n",
    " \n",
    "    \\copy Frequency (frequency,abbrev) FROM '04-at20g-short-frequencies.csv' WITH CSV HEADER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Creation, Part 2: Variabilities Table\n",
    "\n",
    "Psql's <tt>\\copy</tt> command is quite useful - as long as table and CSV files directly match, and as long as the CSV file's content is in good shape. Otherwise it soon reaches its limits.\n",
    "\n",
    "For example lets try using <tt>\\copy</tt> for loading the next file with variability data.\n",
    "We first have to create a new table again. Enter the following SQL command at the psql prompt to create a new table:\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS Variabilities (\n",
    "              source   VARCHAR(20) PRIMARY KEY,\n",
    "              oct04_flux FLOAT,\n",
    "              oct04_err  FLOAT,\n",
    "              oct05_flux FLOAT,\n",
    "              oct05_err  FLOAT,\n",
    "              apr06_flux FLOAT,\n",
    "              apr06_err  FLOAT\n",
    "    );\n",
    "    \n",
    "Check whether the table has been created correctly:\n",
    "\n",
    "        \\d Variabilities\n",
    "        \n",
    "You should see the following:\n",
    "![Variability Table Schema](http://www.it.usyd.edu.au/~roehm/teaching/comp5310/screenshot_postgresql_table-variability \"Variability Table Schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV File Loading, Part 2: Variability Data \n",
    "\n",
    "Next try to load the corresponding CSV data:\n",
    "\n",
    "    \\copy Variabilities FROM '04-at20g-short-variabilities.csv' WITH CSV HEADER\n",
    "\n",
    "Unfortunately, this will result in an error:\n",
    "\n",
    "![Variability Loading Error](http://www.it.usyd.edu.au/~roehm/teaching/comp5310/screenshot_postgresql_load-variabilities-error.jpg \"Variability Loading Error\")\n",
    "\n",
    "What has happened?\n",
    "\n",
    "If you look at the raw '04-at20g-short-variabilities.csv' CSV file, you will find quite a few 'x' entries for some of the variability measurements. These 'x' entries are a problem for <tt>\\copy</tt> because it does not know how to convert those to valid FLOAT numbers, or whether it should replace them with NULL. \n",
    "\n",
    "The only option we have with <tt>\\copy</tt> is at this moment to define that those 'x' entries should be replace with the special NULL value of SQL. You can do so with the **NULL** option:\n",
    "\n",
    "    \\copy Variabilities FROM '04-at20g-short-variabilities.csv' WITH CSV HEADER NULL 'x'\n",
    "    \n",
    "If you run this command, you tell <tt>\\copy</tt> to ignore all 'x' in the CSV file and replace them with a NULL entry instead. This works fine now. You can check you success with the following SQL query after the correct <tt>\\copy</tt> command:\n",
    "\n",
    "    SELECT * FROM Variabilities;\n",
    "    \n",
    "![Variability Loading Ok](http://www.it.usyd.edu.au/~roehm/teaching/comp5310/screenshot_postgresql_load-variabilities-OK.jpg \"Variability Loading Ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks already quite promising, but note a few shortcomings of this approach with <tt>\\copy</tt>:\n",
    "- The CSV columns have to match 1:1 the table schema in the database\n",
    "- We can replace mismatching entries with NULL, but nothing else (eg. no NaN for not-a-number)\n",
    "- We can only replace one well-defined data mismatch, not multiple\n",
    "- There is no mechanism to call a user-defined conversion function for such data where we need to convert it first\n",
    "\n",
    "Basically <tt>\\copy</tt> is a very good and fast approach to load well-formed data, such as a previous database export, into a PostgreSQL database. It does not help us if the data is not so well behaved, or if we have to split and load data into separate tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP PLEASE. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 2: Loading Data into a Database via Python\n",
    "\n",
    "**Next we are back to Python.**\n",
    "\n",
    "We continue with the same Python environment than last week: the `DictReader` from the `csv` module which support reading and writing of files in comma-separated values (CSV).\n",
    "\n",
    "Make sure that you have uploaded the 'at20g-short-frequencies.csv' CSV file into Jupyter.\n",
    "We will first load the content of this file into Python with the same  csv.DictReader()  mechanism than last week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pprint\n",
    "data_frequencies = list(csv.DictReader(open('04-at20g-short-frequencies.csv')))\n",
    "pprint.pprint(data_frequencies[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to execute some SQL statements against the database. As we will have to do so multiple times, we write a dedicated function for executing an arbitrary SQL statement, where we do not expect any result. This handles then also all failures and using psycopg2's 'with' statements also the transaction processing of the database. Below's code will for example automatically commit our SQL statements, as well as rollback if there was any error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pgexec( conn, sqlcmd, args, msg, silent=False ):\n",
    "   \"\"\" utility function to execute some SQL statement\n",
    "       can take optional arguments to fill in (dictionary)\n",
    "       error and transaction handling built-in \"\"\"\n",
    "   retval = False\n",
    "   with conn:\n",
    "      with conn.cursor() as cur:\n",
    "         try:\n",
    "            if args is None:\n",
    "               cur.execute(sqlcmd)\n",
    "            else:\n",
    "               cur.execute(sqlcmd, args)\n",
    "            if silent == False: \n",
    "                print(\"success: \" + msg)\n",
    "            retval = True\n",
    "         except Exception as e:\n",
    "            if silent == False: \n",
    "                print(\"db error: \")\n",
    "                print(e)\n",
    "   return retval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above's function ignores any query result from the SQL statement executed. To be able to also query the content of the database, we introduce another utility function which again encapsulates all error and transaction handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pgquery( conn, sqlcmd, args, silent=False ):\n",
    "   \"\"\" utility function to execute some SQL query statement\n",
    "       can take optional arguments to fill in (dictionary)\n",
    "       will print out on screen the result set of the query\n",
    "       error and transaction handling built-in \"\"\"\n",
    "   retval = False\n",
    "   with conn:\n",
    "      with conn.cursor() as cur:\n",
    "         try:\n",
    "            if args is None:\n",
    "                cur.execute(sqlcmd)\n",
    "            else:\n",
    "                cur.execute(sqlcmd, args)\n",
    "            if silent == False:\n",
    "                for record in cur:\n",
    "                    print(record)\n",
    "            retval = True\n",
    "         except Exception as e:\n",
    "            if silent == False:\n",
    "                print(\"db read error: \")\n",
    "                print(e)\n",
    "   return retval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For larger data sets, the following would normally be executed as a stand alone Python program on a shell.\n",
    "First, you need to establish a connection to the postgresql database. \n",
    "Please edit the database name in below's code to match your Jupyter login."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "DATABASENAME = '<your login name>'  # please replace with your own Jupyter login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    conn = psycopg2.connect(database=DATABASENAME)\n",
    "    print('connected')\n",
    "except Exception as e:\n",
    "    print(\"unable to connect to the database\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's check whether this has all worked fine by querying our PostgreSQL database again.\n",
    "You of course can go back to the Terminal page and in pgsql simply type   SELECT * FROM Frequency\n",
    "\n",
    "Or we do it here in Python again.\n",
    "Using our previous pgquery() function, we query the new Frequency table and simply print out all tuples found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# check content of Frequency table\n",
    "query_stmt = \"SELECT * FROM Frequency\"\n",
    "print(query_stmt)\n",
    "pgquery (conn, query_stmt, None)\n",
    "\n",
    "# cleanup... so that we are not running out of connections on our PostgreSQL server\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Task: Data Loading\n",
    "\n",
    "Try to create and load the Variabilities table.\n",
    "We have provided a CSV file with the corresponding data of measurement fluctuations for a small subset of the astronomical sources. Your task is three-fold:\n",
    " 1. Create a matching 'Variability' table to hold the CSV data\n",
    " 2. Load the content of the csv file into a local 'data_variabilities' dictionary in Python\n",
    " 3. Load the data from the 'data_variabilities' dictionary into your PostgreSQL table\n",
    " 4. Query and print its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pprint\n",
    "data_variabilities = list(csv.DictReader(open('04-at20g-short-variabilities.csv')))\n",
    "pprint.pprint(data_variabilities[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load our previous data.\n",
    "Important: whenever you use this approach, make sure that the header line of your CSV file has no spaces in its column titles and also no quotes. Otherwise, the csv.DictReader might be fine to read it, but not the psycopg2's cursor.execute() function. We are using named placeholders in out INSERT statement below (eg. '%(frequency_mhz)s' ) which expects to put a string (%s) into that place of the INSERT statement as been found in the given dictionary for the execute() call with the key 'frequency_mhz'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    conn = psycopg2.connect(database=DATABASENAME)\n",
    "    print('connected')\n",
    "except Exception as e:\n",
    "    print(\"unable to connect to the database\")\n",
    "    print(e)\n",
    "    \n",
    "# 1st ensure that the schema is in place\n",
    "#\n",
    "table_schema = \"\"\"CREATE TABLE IF NOT EXISTS Variabilities (\n",
    "              source   VARCHAR(20) PRIMARY KEY,\n",
    "              oct04_flux FLOAT,\n",
    "              oct04_err  FLOAT,\n",
    "              oct05_flux FLOAT,\n",
    "              oct05_err  FLOAT,\n",
    "              apr06_flux FLOAT,\n",
    "              apr06_err  FLOAT\n",
    "            )\"\"\"\n",
    "pgexec (conn, table_schema, None, \"Create Table Variabilities\")\n",
    "\n",
    "# 2nd: load data\n",
    "# IMPORTANT: make sure the header line of CSV is without spaces!\n",
    "insert_stmt = \"\"\"INSERT INTO Variabilities(source,oct04_flux,oct04_err,oct05_flux,oct05_err,apr06_flux,apr06_err)\n",
    "                     VALUES (%(source)s, %(oct04_flux)s, %(oct04_err)s, %(oct05_flux)s, %(oct05_err)s, %(apr06_flux)s, %(apr06_err)s)\"\"\"\n",
    "for row in data_variabilities:\n",
    "    pgexec (conn, insert_stmt, row, \"row inserted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check content of Frequency table\n",
    "query_stmt = \"SELECT * FROM Variabilities\"\n",
    "print(query_stmt)\n",
    "pgquery (conn, query_stmt, None)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP PLEASE. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 3: Data Cleaning\n",
    "\n",
    "### Data Cleaning\n",
    "We re-use the iter_clean() function from last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "DEFAULT_VALUE = np.nan\n",
    "def iter_clean(data, column_key, convert_function, default_value):\n",
    "    for row in data:\n",
    "        old_value = row[column_key]\n",
    "        new_value = default_value\n",
    "        try:\n",
    "            new_value = convert_function(old_value)\n",
    "        except (ValueError, TypeError):\n",
    "            warnings.warn('Replacing {} with {} in column {}'.format(\n",
    "                row[column_key], new_value, column_key))\n",
    "        row[column_key] = new_value\n",
    "        yield row\n",
    "\n",
    "# this conversion strips any leading or trailing spaces from the 'frequency_mhz' values\n",
    "data_frequencies = list(iter_clean(data_frequencies, 'frequency_mhz', str.strip, DEFAULT_VALUE))\n",
    "\n",
    "# the following converts the two frequency columns to int values - or NaN\n",
    "data_frequencies = list(iter_clean(data_frequencies, 'frequency_mhz', int, DEFAULT_VALUE))\n",
    "data_frequencies = list(iter_clean(data_frequencies, 'abbreviated_frequency_ghz', int, DEFAULT_VALUE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Task: Data Cleaning\n",
    "\n",
    "Use above's  iter_clean()  iterator function to clean the other give data set too.\n",
    " 1. Clean the  'data_variabilities'  data set\n",
    " 2. Reload the 'data_variabilities'  dictionary into your database\n",
    " 3. Query the 'Variabilities' table - which difference do you see?\n",
    " \n",
    " 4. If you have time: Do all of the above (reading - cleaning - loading) also for the 'Measurements.csv' data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You might encounter a few warning and error messages.\n",
    "   - If a connection is closed, you have to open the databse connection again first\n",
    "   - If the iter_clean() function returns a warning that some string was replaced with NaN, as long as this is indeed a number attribute, you are Ok to ignore this message. It just tells you that it is doing what it is supposed to do.\n",
    "   - If you try to insert data into an already existing table with data inside, you might get 'duplicate primary key' error messages. Again, you can ignore those for the moment.\n",
    "   - If you want to see who much data is already in your table, use the following SQL query:\n",
    "     -  SELECT COUNT(*) FROM Variability;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_variabilities = list(iter_clean(data_variabilities, 'oct04_flux', float, DEFAULT_VALUE))\n",
    "data_variabilities = list(iter_clean(data_variabilities, 'oct04_err',  float, DEFAULT_VALUE))\n",
    "data_variabilities = list(iter_clean(data_variabilities, 'oct05_flux', float, DEFAULT_VALUE))\n",
    "data_variabilities = list(iter_clean(data_variabilities, 'oct05_err',  float, DEFAULT_VALUE))\n",
    "data_variabilities = list(iter_clean(data_variabilities, 'apr06_flux', float, DEFAULT_VALUE))\n",
    "data_variabilities = list(iter_clean(data_variabilities, 'apr06_err',  float, DEFAULT_VALUE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets insert the cleaned data into the existing table.\n",
    "Note that we had previously already 22 rows loaded into the 'Variability' table. Hence we will get 22 'primary key violation' errors. You can safely ignore those errors for the moment. The important point is that the three additional entries with missing data are now inserted too, so that at the end the Variability table contains 25 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    conn = psycopg2.connect(database=DATABASENAME)\n",
    "    print('connected')\n",
    "except Exception as e:\n",
    "    print(\"unable to connect to the database\")\n",
    "    print(e)\n",
    "\n",
    "# check content of Frequency table\n",
    "query_stmt = \"SELECT COUNT(*) FROM Variabilities\"\n",
    "print(query_stmt)\n",
    "pgquery (conn, query_stmt, None)\n",
    "\n",
    "# Try again to load data - 3 more tupels hould now go in\n",
    "insert_stmt = \"\"\"INSERT INTO Variabilities(source,oct04_flux,oct04_err,oct05_flux,oct05_err,apr06_flux,apr06_err)\n",
    "                     VALUES (%(source)s, %(oct04_flux)s, %(oct04_err)s, %(oct05_flux)s, %(oct05_err)s, %(apr06_flux)s, %(apr06_err)s)\"\"\"\n",
    "for row in data_variabilities:\n",
    "    pgexec (conn, insert_stmt, row, \"row inserted\")\n",
    "\n",
    "# check content of Frequency table\n",
    "query_stmt = \"SELECT COUNT(*) FROM Variabilities\"\n",
    "print(query_stmt)\n",
    "pgquery (conn, query_stmt, None)\n",
    "\n",
    "conn.close();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally, also loading the other CSV files\n",
    "data_maincatalogue = list(csv.DictReader(open('04-at20g-short-main-catalogue.csv')))\n",
    "#pprint.pprint(data_maincatalogue[0])\n",
    "\n",
    "data_epochs = list(csv.DictReader(open('04-at20g-short-epochs.csv')))\n",
    "#pprint.pprint(data_epochs[0])\n",
    "\n",
    "data_observations = list(csv.DictReader(open('04-at20g-short-observations.csv')))\n",
    "#pprint.pprint(data_observations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP PLEASE. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 3: Data Modelling & DB Creation\n",
    "In the next exercise we look at modelling the data correctly for a normalised relational star schema. We will first do this as a break-out exercise, then after about 10 minutes collect our findings and discuss an example solution for the given astronomy data model.\n",
    "\n",
    "## Your Task: Data Modelling\n",
    "With your colleagues at the same table, have a look at the complete Excel workbook and discuss how you would model its content using either UML or the Entity Relationship Model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 4: Database Creation\n",
    "After we discussed the model, we will give put an example solution.\n",
    "The next step is to create the corresponding SQL schema in your PostgreSQL database.\n",
    "\n",
    "### Your Task: DB Creation in PostgreSQL\n",
    "Create the corresponding tables in PostgreSQL which follow from the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<pre>\n",
    "CREATE TABLE IF NOT EXISTS FrequencyBand (\n",
    "   band       INTEGER,\n",
    "   frequency1 FLOAT,\n",
    "   frequency2 FLOAT,\n",
    "   CONSTRAINT FrequencyBandPK PRIMARY KEY(band),\n",
    "   CONSTRAINT FrequencyBandU1 UNIQUE (frequency1),\n",
    "   CONSTRAINT FrequencyBandU2 UNIQUE (frequency2)\n",
    "   /* further potential integrity constraints: frequencyX >0 and < ... */\n",
    "   /* alternative: FrequencyBand(band PK, frequency PK) */\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS  Galaxy (\n",
    "   gid  VARCHAR(20),\n",
    "   ra   VARCHAR(11),\n",
    "   dec  VARCHAR(11),\n",
    "   CONSTRAINT GalaxyPK PRIMARY KEY(gid)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS  TelescopeConfig (\n",
    "   configID   INTEGER,\n",
    "   minDec     INTEGER,\n",
    "   maxDec     INTEGER,\n",
    "   frequency1 FLOAT,\n",
    "   frequency2 FLOAT,\n",
    "   tele_array VARCHAR(4),\n",
    "   baseline   VARCHAR(5),\n",
    "   CONSTRAINT TelescopeConfigPK  PRIMARY KEY(configID),\n",
    "   CONSTRAINT TelescopeConfigArrayCHK CHECK (tele_array IN ('1.5B','1.5C','1.5D','H168','H214','H75'))\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS  Epoch (\n",
    "   epochID   INTEGER,\n",
    "   band      INTEGER CHECK (band IN (5,8,20)),\n",
    "   config    INTEGER,\n",
    "   startDate DATE,\n",
    "   endDate   DATE,\n",
    "   CONSTRAINT Epoch_PK  PRIMARY KEY (epochID, band),\n",
    "   CONSTRAINT Epoch_FK1 FOREIGN KEY (band)   REFERENCES FrequencyBand,\n",
    "   CONSTRAINT Epoch_FK2 FOREIGN KEY (config) REFERENCES TelescopeConfig,\n",
    "   CONSTRAINT EpochDates_CHK CHECK (startDate <= endDate)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS  EpochShorthand (\n",
    "   epoch INTEGER,\n",
    "   band  INTEGER,\n",
    "   abbrv VARCHAR(10),\n",
    "   CONSTRAINT EpochShorthandPK PRIMARY KEY(epoch,band,abbrv),\n",
    "   CONSTRAINT EpochShorthandFK FOREIGN KEY(epoch,band) REFERENCES Epoch\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS  Measurement (\n",
    "   gid          VARCHAR(20),\n",
    "   epoch        INTEGER,\n",
    "   band         INTEGER,\n",
    "   intensity    INTEGER,\n",
    "   error        VARCHAR(10), -- INT does not work?\n",
    "   polarisation VARCHAR(6),\n",
    "   polError     VARCHAR(2),\n",
    "   polFrac      VARCHAR(6),\n",
    "   polAngle     VARCHAR(4),\n",
    "   CONSTRAINT MeasurementPK  PRIMARY KEY (gid,epoch,band),\n",
    "   CONSTRAINT MeasurementFK1 FOREIGN KEY (epoch,band) REFERENCES Epoch,\n",
    "   CONSTRAINT MeasurementFK2 FOREIGN KEY (band) REFERENCES FrequencyBand,\n",
    "   CONSTRAINT MeasurementFK3 FOREIGN KEY (gid) REFERENCES Galaxy\n",
    ");\n",
    "\n",
    "DROP TABLE IF EXISTS Variability;\n",
    "CREATE TABLE IF NOT EXISTS Variability (\n",
    "   gid       VARCHAR(20),\n",
    "   epoch     INTEGER,\n",
    "   band      INTEGER,\n",
    "   intensity FLOAT,\n",
    "   error     FLOAT,\n",
    "   CONSTRAINT VariabilityPK  PRIMARY KEY (gid,epoch,band),\n",
    "   CONSTRAINT VariabilityFK1 FOREIGN KEY (epoch,band) REFERENCES Epoch,\n",
    "   CONSTRAINT VariabilityFK2 FOREIGN KEY (band) REFERENCES FrequencyBand,\n",
    "   CONSTRAINT VariabilityFK3 FOREIGN KEY (gid) REFERENCES Galaxy\n",
    ");\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1st ensure that the schema is in place\n",
    "#\n",
    "table_schema = \"\"\"CREATE TABLE IF NOT EXISTS  Galaxy (\n",
    "   gid  VARCHAR(20),\n",
    "   ra   VARCHAR(11),\n",
    "   dec  VARCHAR(11),\n",
    "   CONSTRAINT GalaxyPK PRIMARY KEY(gid)\n",
    ")\"\"\"\n",
    "\n",
    "try: \n",
    "    conn = psycopg2.connect(database=DATABASENAME)\n",
    "    print('connected')\n",
    "    pgexec (conn, table_schema, None, \"Create Table Galaxy\")\n",
    "    conn.close();\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE SOLUTION\n",
    "\n",
    "The following Python fragment creates the whole schema in the PostgreSQL database.\n",
    "Luckily we can execute multiple SQL statements with one pgexec() call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table_schema = \"\"\"CREATE TABLE IF NOT EXISTS FrequencyBand (\n",
    "   band       INTEGER,\n",
    "   frequency1 FLOAT,\n",
    "   frequency2 FLOAT,\n",
    "   CONSTRAINT FrequencyBandPK PRIMARY KEY(band),\n",
    "   CONSTRAINT FrequencyBandU1 UNIQUE (frequency1),\n",
    "   CONSTRAINT FrequencyBandU2 UNIQUE (frequency2)\n",
    "   /* further potential integrity constraints: frequencyX >0 and < ... */\n",
    "   /* alternative: FrequencyBand(band PK, frequency PK) */\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS  Galaxy (\n",
    "   gid  VARCHAR(20),\n",
    "   ra   VARCHAR(11),\n",
    "   dec  VARCHAR(11),\n",
    "   CONSTRAINT GalaxyPK PRIMARY KEY(gid)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS  TelescopeConfig (\n",
    "   configID   INTEGER,\n",
    "   minDec     INTEGER,\n",
    "   maxDec     INTEGER,\n",
    "   frequency1 FLOAT,\n",
    "   frequency2 FLOAT,\n",
    "   tele_array VARCHAR(4),\n",
    "   baseline   VARCHAR(5),\n",
    "   CONSTRAINT TelescopeConfigPK  PRIMARY KEY(configID),\n",
    "   CONSTRAINT TelescopeConfigArrayCHK CHECK (tele_array IN ('1.5B','1.5C','1.5D','H168','H214','H75'))\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS  Epoch (\n",
    "   epochID   INTEGER,\n",
    "   band      INTEGER CHECK (band IN (5,8,20)),\n",
    "   config    INTEGER,\n",
    "   startDate DATE,\n",
    "   endDate   DATE,\n",
    "   CONSTRAINT Epoch_PK  PRIMARY KEY (epochID, band),\n",
    "   CONSTRAINT Epoch_FK1 FOREIGN KEY (band)   REFERENCES FrequencyBand,\n",
    "   CONSTRAINT Epoch_FK2 FOREIGN KEY (config) REFERENCES TelescopeConfig,\n",
    "   CONSTRAINT EpochDates_CHK CHECK (startDate <= endDate)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS  EpochShorthand (\n",
    "   epoch INTEGER,\n",
    "   band  INTEGER,\n",
    "   abbrv VARCHAR(10),\n",
    "   CONSTRAINT EpochShorthandPK PRIMARY KEY(epoch,band,abbrv),\n",
    "   CONSTRAINT EpochShorthandFK FOREIGN KEY(epoch,band) REFERENCES Epoch\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS  Measurement (\n",
    "   gid          VARCHAR(20),\n",
    "   epoch        INTEGER,\n",
    "   band         INTEGER,\n",
    "   intensity    INTEGER,\n",
    "   error        VARCHAR(10), -- INT does not work?\n",
    "   polarisation VARCHAR(6),\n",
    "   polError     VARCHAR(2),\n",
    "   polFrac      VARCHAR(6),\n",
    "   polAngle     VARCHAR(4),\n",
    "   CONSTRAINT MeasurementPK  PRIMARY KEY (gid,epoch,band),\n",
    "   CONSTRAINT MeasurementFK1 FOREIGN KEY (epoch,band) REFERENCES Epoch,\n",
    "   CONSTRAINT MeasurementFK2 FOREIGN KEY (band) REFERENCES FrequencyBand,\n",
    "   CONSTRAINT MeasurementFK3 FOREIGN KEY (gid) REFERENCES Galaxy\n",
    ");\n",
    "\n",
    "DROP TABLE IF EXISTS Variability;\n",
    "CREATE TABLE IF NOT EXISTS Variability (\n",
    "   gid       VARCHAR(20),\n",
    "   epoch     INTEGER,\n",
    "   band      INTEGER,\n",
    "   intensity FLOAT,\n",
    "   error     FLOAT,\n",
    "   CONSTRAINT VariabilityPK  PRIMARY KEY (gid,epoch,band),\n",
    "   CONSTRAINT VariabilityFK1 FOREIGN KEY (epoch,band) REFERENCES Epoch,\n",
    "   CONSTRAINT VariabilityFK2 FOREIGN KEY (band) REFERENCES FrequencyBand,\n",
    "   CONSTRAINT VariabilityFK3 FOREIGN KEY (gid) REFERENCES Galaxy\n",
    ");\"\"\"\n",
    "\n",
    "try: \n",
    "    conn = psycopg2.connect(database=DATABASENAME)\n",
    "    print('connected')\n",
    "    pgexec (conn, table_schema, None, \"Created New Schema\")\n",
    "    conn.close();\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP PLEASE. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 5: Data Loading and Storage\n",
    "\n",
    "Up-to this point, we have\n",
    " - analysed and modelled the given data set\n",
    " - created a corresponding relational star schema\n",
    " - read the individual CSV files into Python dictionary data structures\n",
    " - cleaned the raw data with regard to missing or inconsistent entries and data types\n",
    " \n",
    "The final step is to load this cleaned data into the corresponding tables of the star schema which we defined so far.\n",
    "\n",
    "For this to work, you probably will need to write some logic to load different parts of different data dictionaries (holding the content of CSV files) into different tables.\n",
    "For example, the first few attributes of the 'main catalogue' table define the galaxy entities, hence have to go into the Galaxy table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make sure we loaded the CSV file with the main catalogue\n",
    "import csv\n",
    "import pprint\n",
    "data_maincatalogue = list(csv.DictReader(open('04-at20g-short-main-catalogue.csv')))\n",
    "\n",
    "# connect to the database\n",
    "import psycopg2\n",
    "try: \n",
    "    conn = psycopg2.connect(database=DATABASENAME)\n",
    "    print('connected')\n",
    "except Exception as e:\n",
    "    print(\"unable to connect to the database\")\n",
    "    print(e)\n",
    "\n",
    "# check for any existing content of the Galaxy table\n",
    "query_stmt = \"SELECT COUNT(*) FROM Galaxy\"\n",
    "print(query_stmt)\n",
    "pgquery (conn, query_stmt, None)\n",
    "\n",
    "# Try to load data - 1001 tupels should be created\n",
    "insert_stmt = \"\"\"INSERT INTO Galaxy(gid, ra, dec) VALUES (%(gid)s, %(ra)s, %(dec)s)\"\"\"\n",
    "galaxydata = dict()\n",
    "for row in data_maincatalogue:\n",
    "    galaxydata['gid'] = row['dataset']+row['objectname']\n",
    "    galaxydata['ra']  = row['ra']\n",
    "    galaxydata['dec'] = row['dec']\n",
    "    pgexec (conn, insert_stmt, galaxydata, \"galaxy inserted\")\n",
    "\n",
    "# check content of Galaxy table\n",
    "query_stmt = \"SELECT * FROM Galaxy\"\n",
    "print(query_stmt)\n",
    "pgquery (conn, query_stmt, None)\n",
    "\n",
    "conn.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Task: Data Storage in given Database Schema\n",
    "\n",
    "Take all the three given CSV data sets and load them into the corresponding relational tables. You will need to split some of the table data for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE SOLUTION\n",
    "\n",
    "For convenience, we copy here a few steps which have been done already further up in the notebook, but this way are close together to demonstrate the complete ETL process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1: Extract\n",
    "First, we 'extract' the data from the given CSV files into Python dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "data_frequencies   = list(csv.DictReader(open('04-at20g-short-frequencies.csv')))\n",
    "data_observations  = list(csv.DictReader(open('04-at20g-short-observations.csv')))\n",
    "data_maincatalogue = list(csv.DictReader(open('04-at20g-short-main-catalogue.csv')))\n",
    "data_variabilities = list(csv.DictReader(open('04-at20g-short-variabilities.csv')))\n",
    "data_epochs        = list(csv.DictReader(open('04-at20g-short-epochs.csv')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Clean\n",
    "Next, we clean the data from missing data, inconsistent entries and leading/trailing spaces.\n",
    "We start by introducing a utiloity function that converts numbers, but also allows 'smaller than' and 'greater than' measurement expressions - which we keep for the moment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def int_relaxed(value):\n",
    "    if value != None:\n",
    "      if type(value) == str:\n",
    "        value.strip();\n",
    "        if value[0] == '<':\n",
    "            return value;\n",
    "        if value[0] == '>':\n",
    "            return value;\n",
    "        return int(value);\n",
    "    return value;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Frequency data: convert the two frequency columns to int values - or NaN\n",
    "data_frequencies = list(iter_clean(data_frequencies, 'frequency_mhz', int, DEFAULT_VALUE))\n",
    "data_frequencies = list(iter_clean(data_frequencies, 'abbreviated_frequency_ghz', int, DEFAULT_VALUE))\n",
    "\n",
    "# Clean the observation data from leading and trailing spaces around values \n",
    "# we are applying the str.strip (string strip) function to all values of 'baseline', 'array' and 'observing_dates' columns\n",
    "data_observations = list(iter_clean(data_observations, 'baseline', str.strip, None))\n",
    "data_observations = list(iter_clean(data_observations, 'array',    str.strip, None))\n",
    "data_observations = list(iter_clean(data_observations, 'observing_dates', str.strip, None))\n",
    "\n",
    "# Cleaning the measurement data\n",
    "# we keep the '<20' notation values, but note that due to our schema choices for the Measurement table\n",
    "# we need to use None (NULL) rather than NaN (not supported for integer columns)\n",
    "# and we also transform all polarisation values, if not correct, to None (NULL)\n",
    "data_frequencies = list(iter_clean(data_maincatalogue, 'S5',    int_relaxed, None))\n",
    "data_frequencies = list(iter_clean(data_maincatalogue, 'err5',  int_relaxed, DEFAULT_VALUE))\n",
    "data_frequencies = list(iter_clean(data_maincatalogue, 'P5',    int_relaxed, None))\n",
    "data_frequencies = list(iter_clean(data_maincatalogue, 'P5err', int_relaxed, None))\n",
    "data_frequencies = list(iter_clean(data_maincatalogue, 'M5',    int_relaxed, None))\n",
    "data_frequencies = list(iter_clean(data_maincatalogue, 'M5pa',  int_relaxed, None))\n",
    "data_frequencies = list(iter_clean(data_maincatalogue, 'S8',    int_relaxed, None))\n",
    "data_frequencies = list(iter_clean(data_maincatalogue, 'err8',  int_relaxed, DEFAULT_VALUE))\n",
    "data_frequencies = list(iter_clean(data_maincatalogue, 'P8',    int_relaxed, None))\n",
    "data_frequencies = list(iter_clean(data_maincatalogue, 'P8err', int_relaxed, None))\n",
    "data_frequencies = list(iter_clean(data_maincatalogue, 'M8',    int_relaxed, None))\n",
    "data_frequencies = list(iter_clean(data_maincatalogue, 'M8pa',  int_relaxed, None))\n",
    "data_frequencies = list(iter_clean(data_maincatalogue, 'S20',   int_relaxed, None))\n",
    "data_frequencies = list(iter_clean(data_maincatalogue, 'err20', int_relaxed, DEFAULT_VALUE))\n",
    "data_frequencies = list(iter_clean(data_maincatalogue, 'P20',   int_relaxed, None))\n",
    "data_frequencies = list(iter_clean(data_maincatalogue, 'P20err',int_relaxed, None))\n",
    "data_frequencies = list(iter_clean(data_maincatalogue, 'M20',   int_relaxed, None))\n",
    "data_frequencies = list(iter_clean(data_maincatalogue, 'M20pa', int_relaxed, None))\n",
    "\n",
    "# Cleaning the variabilities data\n",
    "data_variabilities = list(iter_clean(data_variabilities, 'oct04_flux', float, DEFAULT_VALUE))\n",
    "data_variabilities = list(iter_clean(data_variabilities, 'oct04_err',  float, DEFAULT_VALUE))\n",
    "data_variabilities = list(iter_clean(data_variabilities, 'oct05_flux', float, DEFAULT_VALUE))\n",
    "data_variabilities = list(iter_clean(data_variabilities, 'oct05_err',  float, DEFAULT_VALUE))\n",
    "data_variabilities = list(iter_clean(data_variabilities, 'apr06_flux', float, DEFAULT_VALUE))\n",
    "data_variabilities = list(iter_clean(data_variabilities, 'apr06_err',  float, DEFAULT_VALUE))\n",
    "\n",
    "# Cleaning the epoch shorthand data\n",
    "# we could replace all '-' with None, but we decided to do it in the load function later directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 3: Transform and Load\n",
    "For the loading phase, we first introduce another utility function which will help us retrieving individual values from postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pgqueryval( conn, sqlcmd, args, silent=False ):\n",
    "   \"\"\" utility function to execute some SQL query statement\n",
    "       it can take optional arguments to fill in (dictionary)\n",
    "       returns the first result record from the query (any more than the 1st will be ignored)\n",
    "       error and transaction handling built-in \"\"\"\n",
    "   retval = None\n",
    "   with conn:\n",
    "      with conn.cursor() as cur:\n",
    "         try:\n",
    "            if args is None:\n",
    "                cur.execute(sqlcmd)\n",
    "            else:\n",
    "                cur.execute(sqlcmd, args)\n",
    "            retval = cur.fetchone()\n",
    "         except Exception as e:\n",
    "            if silent == False:\n",
    "                print(\"db read error: \")\n",
    "                print(e)\n",
    "   return retval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SubTask 1: Loading Frequencies.csv\n",
    "\n",
    "The data of the '04-at20g-short-frequencies.csv' CSV file be loaded into the <tt>FrequencyBand</tt> table. \n",
    "\n",
    "The transformation step here is that dual-entries for the same frequency band will be merged into the same row, but separate columns ('frequency1' and 'frequency2')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# connect to the database\n",
    "import psycopg2\n",
    "try: \n",
    "    conn = psycopg2.connect(database=DATABASENAME)\n",
    "    print('connected')\n",
    "except Exception as e:\n",
    "    print(\"unable to connect to the database\")\n",
    "    print(e)\n",
    "\n",
    "# cleanup of any potential previous left-over-rows in the table\n",
    "pgexec (conn, \"DELETE FROM FrequencyBand\", None, \"FrequencyBand cleared\", True)\n",
    "\n",
    "# load data from the frequencies dictionary\n",
    "insert_stmt_FB = \"\"\"INSERT INTO FrequencyBand VALUES (%(band)s, %(freq)s, NULL)\"\"\"\n",
    "update_stmt_FB = \"\"\"UPDATE FrequencyBand SET frequency2 = %(freq)s WHERE band=%(band)s\"\"\"\n",
    "\n",
    "fbdata= dict()\n",
    "for row in data_frequencies:\n",
    "    fbdata['band'] = row['abbreviated_frequency_ghz']\n",
    "    fbdata['freq'] = row['frequency_mhz']\n",
    "    success = pgexec (conn, insert_stmt_FB, fbdata, \"FrequencyBand loaded\", True)\n",
    "    if success == False:\n",
    "        pgexec (conn, update_stmt_FB, fbdata, \"FrequencyBand updated\")\n",
    "\n",
    "# check content of FrequencyBand table\n",
    "query_stmt = \"SELECT * FROM FrequencyBand\"\n",
    "print(query_stmt)\n",
    "pgquery (conn, query_stmt, None)\n",
    "\n",
    "conn.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SubTask 2: Loading Observations.csv\n",
    "\n",
    "The data of the '04-at20g-short-observations.csv' CSV file gets split into two separate tables: the **<tt>TelescopeConfig</tt>** and the **<tt>Epoch</tt>** tables. \n",
    "\n",
    "While the entries for <tt>TelescopeConfig</tt> are relative straight-forward, we need to apply more transformations to the observation data. For example, start and end dates are now separately stored and need to get extracted from the interval string of the CSV file. Also some observations are made in two epochs, in which case we also transpose the two epoch values into two separate <tt>Epoch</tt> rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# re-connect to the database\n",
    "import psycopg2\n",
    "try: \n",
    "    conn = psycopg2.connect(database=DATABASENAME)\n",
    "    print('connected')\n",
    "except Exception as e:\n",
    "    print(\"unable to connect to the database\")\n",
    "    print(e)\n",
    "\n",
    "# cleanup of any potential previous left-overs in the two tables\n",
    "delete_stmt_T = \"\"\"DELETE FROM TelescopeConfig\"\"\"\n",
    "delete_stmt_E = \"\"\"DELETE FROM Epoch\"\"\"\n",
    "pgexec (conn, delete_stmt_T, None, \"TelescopeConfig cleared\", True)\n",
    "pgexec (conn, delete_stmt_E, None, \"Epoch cleared\", True)\n",
    "\n",
    "# TRANSFORM AND LOAD data into both tables\n",
    "# we first define the two insert statements for later use\n",
    "insert_stmt_T = \"\"\"INSERT INTO TelescopeConfig(configID, minDec, maxDec, frequency1, frequency2, tele_array, baseline) \n",
    "                   VALUES (%(id)s, %(mindec)s, %(maxdec)s, %(freq1)s, %(freq2)s, %(tarray)s, %(bline)s)\"\"\"\n",
    "insert_stmt_E = \"\"\"INSERT INTO Epoch(epochID, band, config, startDate, endDate) VALUES (%(id)s, %(band)s, %(confg)s, %(sdate)s, %(edate)s)\"\"\"\n",
    "\n",
    "# next we iterate through the whole observation data and convert the raw CSV file into the correct values needed by the SQL INSERT statement\n",
    "configdata= dict()\n",
    "epochdata = dict()\n",
    "for row in data_observations:\n",
    "    # compute new config id based on extisting values + 1\n",
    "    configId = pgqueryval(conn, \"SELECT COALESCE(MAX(configID),0)+1 FROM TelescopeConfig\", None)\n",
    "    \n",
    "    # transform observation data into one TelescopeConfig entry\n",
    "    configdata['id']    = configId[0]\n",
    "    configdata['mindec']= row['min_declination']\n",
    "    configdata['maxdec']= row['max_declination']\n",
    "    configdata['freq1'] = row['frequency1']\n",
    "    configdata['freq2'] = row['frequency2']\n",
    "    configdata['tarray']= row['array']\n",
    "    configdata['bline'] = row['baseline']\n",
    "    pgexec (conn, insert_stmt_T, configdata, \"TelescopeConfig inserted\")\n",
    "\n",
    "    # transform observation data also into one or two Epoch entries\n",
    "    startdate = row['observing_dates'][0:6]+row['observing_dates'][-5:]\n",
    "    enddate   = row['observing_dates'][-11:]\n",
    "    if row['frequency1'] == '4800':\n",
    "        epochdata['id']    = row['epoch']\n",
    "        epochdata['band']  = 5\n",
    "        epochdata['confg'] = configId\n",
    "        epochdata['sdate'] = startdate\n",
    "        epochdata['edate'] = enddate\n",
    "        pgexec (conn, insert_stmt_E, epochdata, \"epoch inserted\")\n",
    "        epochdata['id']    = row['epoch']\n",
    "        epochdata['band']  = 8\n",
    "        epochdata['confg'] = configId\n",
    "        epochdata['sdate'] = startdate\n",
    "        epochdata['edate'] = enddate\n",
    "        pgexec (conn, insert_stmt_E, epochdata, \"epoch inserted\")\n",
    "    else:\n",
    "        epochdata['id']    = row['epoch']\n",
    "        epochdata['band']  = 20\n",
    "        epochdata['confg'] = configId\n",
    "        epochdata['sdate'] = startdate\n",
    "        epochdata['edate'] = enddate\n",
    "        pgexec (conn, insert_stmt_E, epochdata, \"epoch inserted\")\n",
    "\n",
    "# check final content of TelescopeConfig table\n",
    "query_stmt = \"SELECT * FROM TelescopeConfig\"\n",
    "print(query_stmt)\n",
    "pgquery (conn, query_stmt, None)\n",
    "\n",
    "# check final content of Epoch table\n",
    "query_stmt = \"SELECT * FROM Epoch\"\n",
    "print(query_stmt)\n",
    "pgquery (conn, query_stmt, None)\n",
    "\n",
    "conn.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SubTask 3: Load MainCatalogue.csv\n",
    "\n",
    "Next is the task to load the '04-at20g-short-main-catalogue.csv' file into the <tt>Measurement</tt> table. \n",
    "\n",
    "Most of the work with the measurement data was already done before in the cleaning phase. All which is left here is to combine the first two values of the raw CSV file into the correct galaxy ID value which matches that values from the other tables.\n",
    "\n",
    "Note that we did create already the entries in the <tt>Galaxy</tt> table in our example loading code as part of the Tutorial. Otherwise this would be part of this task here too.\n",
    "\n",
    "The main transformation left here is a **split** and **transpose** operation:\n",
    "Each entry from '04-at20g-short-main-catalogue.csv' gets split into up-to four entries:\n",
    "1. An <tt>Galaxy</tt> entry\n",
    "2. Between one and three entries <tt>Measurement</tt> entries, depending on which frequencies were observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# connect to the database\n",
    "import psycopg2\n",
    "try: \n",
    "    conn = psycopg2.connect(database=DATABASENAME)\n",
    "    print('connected')\n",
    "except Exception as e:\n",
    "    print(\"unable to connect to the database\")\n",
    "    print(e)\n",
    "\n",
    "# cleanup of any potential previous left-overs in the two tables\n",
    "pgexec (conn, \"DELETE FROM Measurement\", None, \"Measurement cleared\", True)\n",
    "\n",
    "# TRANSFORM AND LOAD data into the Measurement table\n",
    "# we first define the insert statement for later use\n",
    "insert_stmt_M = \"\"\"INSERT INTO Measurement(gid, epoch, band, intensity, error, polarisation, polError, polFrac, polAngle) \n",
    "                   VALUES (%(galaxy)s, %(epoch)s, %(band)s, %(intensity)s, %(error)s, %(pol)s, %(polError)s, %(polFrac)s, %(polAngle)s)\"\"\"\n",
    "\n",
    "# next we iterate through the measurment data and convert the raw CSV file into the correct values needed by the SQL INSERT statement\n",
    "measurement = dict()\n",
    "for row in data_maincatalogue:\n",
    "    epoch20 = row['epoch'][0:1]\n",
    "    epoch8  = row['epoch'][1:2]\n",
    "    epoch5  = row['epoch'][2:3]\n",
    "    measurement['galaxy']   = row['dataset']+row['objectname']\n",
    "    if epoch5 != '.':\n",
    "        measurement['epoch']    = epoch5\n",
    "        measurement['band']     = 5\n",
    "        measurement['intensity']= row['S5']\n",
    "        measurement['error']    = row['err5']\n",
    "        measurement['pol']      = row['P5']\n",
    "        measurement['polError'] = row['P5err']\n",
    "        measurement['polFrac']  = row['M5']\n",
    "        measurement['polAngle'] = row['M5pa']\n",
    "        pgexec (conn, insert_stmt_M, measurement, \"5GHz measurement inserted\")\n",
    "    if epoch8 != '.':\n",
    "        measurement['epoch']    = epoch8\n",
    "        measurement['band']     = 8\n",
    "        measurement['intensity']= row['S8']\n",
    "        measurement['error']    = row['err8']\n",
    "        measurement['pol']      = row['P8']\n",
    "        measurement['polError'] = row['P8err']\n",
    "        measurement['polFrac']  = row['M8']\n",
    "        measurement['polAngle'] = row['M8pa']\n",
    "        pgexec (conn, insert_stmt_M, measurement, \"8GHz measurement inserted\")\n",
    "    if epoch20 != '.':\n",
    "        measurement['epoch']    = epoch20\n",
    "        measurement['band']     = 20\n",
    "        measurement['intensity']= row['S20']\n",
    "        measurement['error']    = row['err20']\n",
    "        measurement['pol']      = row['P20']\n",
    "        measurement['polError'] = row['P20err']\n",
    "        measurement['polFrac']  = row['M20']\n",
    "        measurement['polAngle'] = row['M20pa']\n",
    "        pgexec (conn, insert_stmt_M, measurement, \"8GHz measurement inserted\")\n",
    "\n",
    "# check content of Measurement table\n",
    "query_stmt = \"SELECT * FROM Measurement\"\n",
    "print(query_stmt)\n",
    "pgquery (conn, query_stmt, None)\n",
    "\n",
    "conn.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SubTask 4: Load Variabilities.csv\n",
    "\n",
    "Next is the task to load the '04-at20g-short-variabilities.csv' file into the new <tt>Variability</tt> table. \n",
    "\n",
    "Note that we did already clean the variabilities data before. All left here is \n",
    "1. a value transformation step that converts the intensity values to milliJanskys to match the values later from the <tt>Measurement</tt> table.\n",
    "2. a **transpose** operation that stores each row of the original CSV file in up-to three rows of the <tt>Variability</tt>? table, one for each frequency band.\n",
    "3. if there is no variablity value for one of those bands - after our initial cleaning step represented by NaN - we simply ignore those entries and do not store them at all in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import numpy as np\n",
    "\n",
    "# connect to the database\n",
    "try: \n",
    "    conn = psycopg2.connect(database=DATABASENAME)\n",
    "    print('connected')\n",
    "except Exception as e:\n",
    "    print(\"unable to connect to the database\")\n",
    "    print(e)\n",
    "\n",
    "# cleanup of any potential previous left-overs in the Variability table\n",
    "pgexec (conn, \"DELETE FROM Variability\", None, \"Variability cleared\", True)\n",
    "\n",
    "# TRANSFORM AND LOAD data into the Variability table\n",
    "# we first define the insert statement for later use\n",
    "insert_stmt_V = \"\"\"INSERT INTO Variability(gid, epoch, band, intensity, error) \n",
    "                   VALUES (%(galaxy)s, %(epoch)s, %(band)s, %(intensity)s, %(error)s)\"\"\"\n",
    "\n",
    "# next we iterate through the variability data and convert the CSV file into the correct values needed by the SQL INSERT statement\n",
    "variability = dict()\n",
    "for row in data_variabilities:\n",
    "    variability['galaxy']   = row['source']\n",
    "    if not( np.isnan(row['oct04_flux']) ):\n",
    "        variability['epoch']    = 1\n",
    "        variability['band']     = 20\n",
    "        variability['intensity']= row['oct04_flux'] * 1000; # convert Jansky value to milliJansky\n",
    "        variability['error']    = row['oct04_err']\n",
    "        pgexec (conn, insert_stmt_V, variability, \"20GHz variability inserted\")\n",
    "    if not( np.isnan(row['oct05_flux']) ):\n",
    "        variability['epoch']    = 2\n",
    "        variability['band']     = 20\n",
    "        variability['intensity']= row['oct05_flux'] * 1000; # convert Jansky value to milliJansky\n",
    "        variability['error']    = row['oct05_err']\n",
    "        pgexec (conn, insert_stmt_V, variability, \"20GHz variability inserted\")\n",
    "    if not( np.isnan(row['apr06_flux']) ):\n",
    "        variability['epoch']    = 3\n",
    "        variability['band']     = 20\n",
    "        variability['intensity']= row['apr06_flux'] * 1000; # convert Jansky value to milliJansky\n",
    "        variability['error']    = row['apr06_err']\n",
    "        pgexec (conn, insert_stmt_V, variability, \"20GHz variability inserted\")\n",
    "\n",
    "# check content of Variability table\n",
    "query_stmt = \"SELECT * FROM Variability\"\n",
    "print(query_stmt)\n",
    "pgquery (conn, query_stmt, None)\n",
    "\n",
    "conn.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SubTask 5: Load Epochs.csv\n",
    "\n",
    "The final task is to load the '04-at20g-short-main-epochs.csv' file into the <tt>EpochsShorthand</tt> table. \n",
    "\n",
    "This is again an example of an **transpose** transformation, in that each row of the original CSV file becomes up-to three rows in the database table. For example, the CSV entry (1, oct04, nov04) will be loaded into three rows that specify the three different date-names used for the three measurement bands 20Ghz, 8Ghz and 5Ghz: \n",
    "\n",
    "    (1, 20, oct04)\n",
    "    (1,  5, oct04)\n",
    "    (1,  8, oct04) \n",
    "\n",
    "Additionally, we also ignore all '-' entries as those represent non-existing data, and hence we simply do no create any entry in our database for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# connect to the database\n",
    "import psycopg2\n",
    "try: \n",
    "    conn = psycopg2.connect(database=DATABASENAME)\n",
    "    print('connected')\n",
    "except Exception as e:\n",
    "    print(\"unable to connect to the database\")\n",
    "    print(e)\n",
    "\n",
    "# cleanup of any potential previous left-overs in the EpochShorthand table\n",
    "pgexec (conn, \"DELETE FROM EpochShorthand\", None, \"EpochShorthand cleared\", True)\n",
    "\n",
    "# TRANSFORM AND LOAD data into the EpochShorthand table\n",
    "# we first define the insert statement for later use\n",
    "insert_stmt_ES = \"\"\"INSERT INTO EpochShorthand(epoch, band, abbrv) \n",
    "                         VALUES (%(epoch)s, %(band)s, %(abbrv)s)\"\"\"\n",
    "\n",
    "# we convert the raw CSV file into the correct values needed by the SQL INSERT statement\n",
    "epochdata = dict()\n",
    "for row in data_epochs:\n",
    "    epochdata['epoch'] = row['epoch']\n",
    "    if row['date_20ghz'] != '-':\n",
    "        epochdata['band']  = 20\n",
    "        epochdata['abbrv'] = row['date_20ghz']\n",
    "        pgexec (conn, insert_stmt_ES, epochdata, \"20GHz epoch shorthand inserted\")\n",
    "    if row['date_5or8gHz'] != '-':\n",
    "        epochdata['band']  = 8\n",
    "        epochdata['abbrv'] = row['date_5or8gHz']\n",
    "        pgexec (conn, insert_stmt_ES, epochdata, \"8GHz epoch shorthand inserted\")\n",
    "        epochdata['band']  = 5\n",
    "        epochdata['abbrv'] = row['date_5or8gHz']\n",
    "        pgexec (conn, insert_stmt_ES, epochdata, \"5GHz epoch shorthand inserted\")\n",
    "\n",
    "# check content of EpochShorthand table\n",
    "query_stmt = \"SELECT * FROM EpochShorthand\"\n",
    "print(query_stmt)\n",
    "pgquery (conn, query_stmt, None)\n",
    "\n",
    "conn.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Tutorial. Many Thanks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
