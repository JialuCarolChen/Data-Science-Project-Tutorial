{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Big Data Analysis\n",
    "\n",
    "This week is about big data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## EXERCISE 1: The Three Vs of Big Data\n",
    "\n",
    "Discuss per table:\n",
    "1. your own experiences with Big Data and which kinds of challenges you or your organisation are facing. \n",
    "2. Are all three Vs required to make a data analytics problem a Big Data problem? \n",
    "3. Can you think of any other characteristics which could be helpful to define the meaning of Big Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# STOP PLEASE. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## EXERCISE 2: Map Reduce Word Count\n",
    "\n",
    "In the following exercise we will look at the principles of Map Reduce programming.<br/>\n",
    "Note that this exercise is not assuming any parallelism or distribution of code, but that everything is running on a single computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The idea behind Map Reduce can be shown on the simple example of Word Count:\n",
    "\n",
    "Given a potentially large collection of text documents, the goal is to determine the number of individual words used in these texts.\n",
    "\n",
    "A straight-foward, procedural approach to solve this problem in Python would be as follows.\n",
    "Note: The following example is taken from \"Data Science from Scratch\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# first we introduce a utility function to tokenize a given document string into a list of words\n",
    "\n",
    "import re\n",
    "\n",
    "def tokenize(message):\n",
    "    message = message.lower()                       # convert to lowercase\n",
    "    all_words = re.findall(\"[a-z0-9']+\", message)   # extract the words\n",
    "    return set(all_words)                           # remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def word_count_old(documents):\n",
    "    \"\"\"word count not using MapReduce\"\"\"\n",
    "    return Counter(word\n",
    "        for document in documents\n",
    "            for word in tokenize(document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's try this out with a short test set of the following three *documents* (a document is simply a sequence of words):\n",
    "\n",
    "<pre>{\"data science\",\n",
    " \"big data\",\n",
    " \"science fiction\"}</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'big': 1, 'data': 2, 'fiction': 1, 'science': 2})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_old({\"data science\", \"big data\", \"science fiction\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This approach reads and counts all word occurences in a sequential way. The more documents and the longer these documents, the longer this approach takes. For a real Big Data setting with large volumes it will however get too slow. Hence we would like to turn this into a parallelisable Map-Reduce approach.\n",
    "\n",
    "For this we need to provide three things:\n",
    "1. a mapper function\n",
    "2. a reducer function\n",
    "3. some control code that manages the data flow from the inputs through the mapper to the reducer\n",
    "\n",
    "**Mapper**. The fist step is to write a mapper function that transforms the given input - in our case a text document - into a set of intermediate (key,value) pairs. For the given WordCount example, we write a *wc_mapper()* function that turns a given document into a sequence of word-count pairs. We chose single words as the key because we need this output in the next step to be grouped by word to be able to determine the overall word count. The mapper will emit for each word just the value 1 to indicate that this pair corresponds to one occurrence of the word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def wc_mapper(document):\n",
    "    \"\"\"for each word in the document, emit (word,1)\"\"\"\n",
    "    for word in tokenize(document):\n",
    "        yield (word, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Reducer.** The reducer expects as input a key and a list of associated values. In our case, the \"plumming\" code (which we will discuss in the next step) will call the reducer with a word as key and a list of all its occurrences counts that the mapper above had emitted. The reducer then produces a single word count for the given word by summing over all the individual counts given:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def wc_reducer(word, counts):\n",
    "    \"\"\"sum up the counts for a word\"\"\"\n",
    "    yield (word, sum(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, we need the \"plumming\" code that controls the data flow between mapper and reducer.\n",
    "Let’s think about how we would do this on just one computer:\n",
    "\n",
    "**Control Code:** The following code loops through all the documentsm calls the *wc_mapper()* for each document and collects all emitted *(word, 1)* in an intermediate collector dictionary. This internal *collector* dictionary data structure does a lot of useful work for us, including groupping counts for the same word together.\n",
    "\n",
    "After mapping all input document, the control code then will go through the collected word-count pairs and call the *wc_reducer()* function for each word with all its counts together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def word_count(documents):\n",
    "    \"\"\"count the words in the input documents using MapReduce\"\"\"\n",
    "\n",
    "    # place to store grouped values\n",
    "    collector = defaultdict(list)\n",
    "\n",
    "    for document in documents:\n",
    "        for word, count in wc_mapper(document):\n",
    "            collector[word].append(count)\n",
    "\n",
    "    return [output\n",
    "            for word, counts in collector.items()\n",
    "            for output in wc_reducer(word, counts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, let's try this code.\n",
    "\n",
    "## YOUR TASK 1\n",
    "\n",
    "Imagine that we have the same three documents as above:\n",
    "<pre> [\"data science\", \"big data\", \"science fiction\"] </pre>\n",
    "\n",
    "1. Write the code to call the *word_count()* on those three documents and document the result.\n",
    "2. Also check what the result of just the *wc_mapper()* is with this input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('science', 1)\n",
      "('data', 1)\n",
      "('big', 1)\n",
      "('data', 1)\n",
      "('science', 1)\n",
      "('fiction', 1)\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "docs = [\"data science\", \"big data\", \"science fiction\"] \n",
    "word_count(docs)\n",
    "for doc in docs:\n",
    "    for x in wc_mapper(doc):\n",
    "        print (x)\n",
    "        \n",
    "#for x in [wc_mapper(doc) for doc in docs]:\n",
    "#    print(next(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### MapReduce more general\n",
    "\n",
    "If you think about it for a minute, all of the word-count-specific code in the previous example is contained in the *wc_mapper* and *wc_reducer* functions. This means that with a couple of changes we have a much more general framework (that still runs on a single machine):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def map_reduce(inputs, mapper, reducer):\n",
    "    \"\"\"runs MapReduce on the inputs using mapper and reducer\"\"\"\n",
    "    collector = defaultdict(list)\n",
    "\n",
    "    for input in inputs:\n",
    "        for key, value in mapper(input):\n",
    "            collector[key].append(value)\n",
    "\n",
    "    return [output\n",
    "            for key, values in collector.items()\n",
    "            for output in reducer(key,values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## YOUR TASK 2\n",
    "\n",
    "Use above's *map_reduce()* function and the previously defined *wc_mapper()* and *wc_reducer()* functions to count all words in the same set of there documents than in the previous task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('science', 2), ('data', 2), ('big', 1), ('fiction', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_reduce ( docs, wc_mapper, wc_reducer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Further Generalisation:** \n",
    "\n",
    "Above's code gives us the flexibility to solve a wide variety of problems.\n",
    "Before we proceed, observe that wc_reducer is just summing the values corresponding to each key. This kind of aggregation is common enough that it’s worth abstracting it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def reduce_with(aggregation_fn, key, values):\n",
    "    \"\"\"reduces a key-values pair by applying aggregation_fn to the values\"\"\"\n",
    "    yield (key, aggregation_fn(values))\n",
    "\n",
    "def values_reducer(aggregation_fn):\n",
    "    \"\"\"turns a function (values -> output) into a reducer\"\"\"\n",
    "    return partial(reduce_with, aggregation_fn)\n",
    "\n",
    "sum_reducer = values_reducer(sum)\n",
    "max_reducer = values_reducer(max)\n",
    "min_reducer = values_reducer(min)\n",
    "count_distinct_reducer = values_reducer(lambda values: len(set(values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## YOUR TASK 3\n",
    "\n",
    "Use the appropriate generalised reducer in above's *map_reduce()* function to count all words in the same set of there documents than in the previous task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('science', 2), ('data', 2), ('big', 1), ('fiction', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "map_reduce(docs, wc_mapper, sum_reducer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# STOP PLEASE. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## EXERCISE 3: More Complex Data Analysis using MapReduce\n",
    "\n",
    "In this next exercise, we want to further generalise the MapReduce approach and use it to analyse data which has a bit more structure than just linear text.\n",
    "\n",
    "**Input Data Format**\n",
    "\n",
    "Assume that we want to analyse a collection of *status updates* which get processed in some web system. Further assume that these status updates are available in JSON format such as the following example:\n",
    "<pre>\n",
    "{\"id\": 1,\n",
    " \"username\" : \"joelgrus\",\n",
    " \"text\" : \"Is anyone interested in a data science book?\",\n",
    " \"created_at\" : datetime.datetime(2013, 12, 21, 11, 47, 0),\n",
    " \"liked_by\" : [\"data_guy\", \"data_gal\", \"mike\"] }</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** Example: Filtering with MapReduce: ** (taken from \"Data Science from Scratch\")\n",
    "\n",
    "\"Let’s say we need to figure out which day of the week people talk the most about data science. In order to find this, we’ll just count how many data science updates there are on each day of the week. This means we’ll need to group by the day of week, so that’s our key. And if we emit a value of 1 for each update that contains “data science,” we can simply get the total number using sum:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def data_science_day_mapper(status_update):\n",
    "    \"\"\"yields (day_of_week, 1) if status_update contains \"data science\" \"\"\"\n",
    "    if \"data science\" in status_update[\"text\"].lower():\n",
    "        day_of_week = status_update[\"created_at\"].weekday()\n",
    "        yield (day_of_week, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## YOUR TASK 4\n",
    "\n",
    "Use above's *data_science_day_mapper()* function to determine the distribution of updates-per-day_of_week for the following list of status updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "status_updates = [\n",
    "    {\"id\": 1,\n",
    " \"username\" : \"joelgrus\",\n",
    " \"text\" : \"Is anyone interested in a data science book?\",\n",
    " \"created_at\" : datetime.datetime(2015, 12, 21, 11, 47, 0),\n",
    " \"liked_by\" : [\"data_guy\", \"data_gal\", \"mike\"] },\n",
    "    {\"id\": 2,\n",
    " \"username\" : \"ben\",\n",
    " \"text\" : \"data science is fun\",\n",
    " \"created_at\" : datetime.datetime(2015, 12, 21, 11, 47, 0),\n",
    " \"liked_by\" : [\"mike\"] },\n",
    "    {\"id\": 3,\n",
    " \"username\" : \"uwe\",\n",
    " \"text\" : \"databases are fun\",\n",
    " \"created_at\" : datetime.datetime(2015, 12, 22, 10, 50, 0),\n",
    " \"liked_by\" : [\"data_gal\"] },\n",
    "    {\"id\": 4,\n",
    " \"username\" : \"ben\",\n",
    " \"text\" : \"spark for data science\",\n",
    " \"created_at\" : datetime.datetime(2015, 12, 23, 11, 17, 0),\n",
    " \"liked_by\" : [\"data_guy\", \"mike\"] },\n",
    "    {\"id\": 5,\n",
    " \"username\" : \"joelgrus\",\n",
    " \"text\" : \"what is this big data thing anyway?\",\n",
    " \"created_at\" : datetime.datetime(2015, 12, 23, 11, 33, 0),\n",
    " \"liked_by\" : [\"data_gal\", \"data_guy\"] },\n",
    "    {\"id\": 6,\n",
    " \"username\" : \"ben\",\n",
    " \"text\" : \"lots of data science, lots of fun\",\n",
    " \"created_at\" : datetime.datetime(2015, 12, 24, 12, 17, 0),\n",
    " \"liked_by\" : [\"mike\"] }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2), (2, 1), (3, 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "map_reduce(status_updates,data_science_day_mapper, sum_reducer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## YOUR TASK 5\n",
    "\n",
    "Modify above's *data_science_day_mapper()* function so that it instead determines the distribution of updates-per-day_of_week for a the user 'ben':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (2, 1), (3, 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "def data_science_day_mapper_for_ben(status_update):\n",
    "    \"\"\"yields (day_of_week, 1) if status_update contains \"data science\" \"\"\"\n",
    "    if \"data science\" in status_update[\"text\"].lower() and status_update[\"username\"].lower() == \"ben\":\n",
    "        day_of_week = status_update[\"created_at\"].weekday()\n",
    "        yield (day_of_week, 1)\n",
    "map_reduce(status_updates,data_science_day_mapper_for_ben, sum_reducer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## YOUR TASK 6\n",
    "\n",
    "Write a new *liker_mapper()* function that determines the number of distinct status-likers for each user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('joelgrus', 3), ('ben', 2), ('uwe', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "def liker_mapper(status_update):\n",
    "    user = status_update[\"username\"]\n",
    "    likers = status_update[\"liked_by\"]\n",
    "    for liker in likers:\n",
    "        yield (user, liker)\n",
    "    \n",
    "map_reduce(status_updates, liker_mapper, count_distinct_reducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Example: Top-K (Sub-)Counting\n",
    "\n",
    "As a more complicated example, assume that we want to find out for each user the most common word that s/he is using in her status updates. In this case, we would like to group by *username* ('for each user') and count the individual words used per user. To do the latter, we tokenize the status update text in the mapper, and in the reducer we count the number of occurences of each word by a given user - and report just he one with the highest count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('joelgrus', ('data', 2)), ('ben', ('science', 3)), ('uwe', ('fun', 1))]\n"
     ]
    }
   ],
   "source": [
    "def words_per_user_mapper(status_update):\n",
    "    user = status_update[\"username\"]\n",
    "    for word in tokenize(status_update[\"text\"]):\n",
    "        yield (user, (word, 1))\n",
    "\n",
    "def most_popular_word_reducer(user, words_and_counts):\n",
    "    \"\"\"given a sequence of (word, count) pairs,\n",
    "    return the word with the highest total count\"\"\"\n",
    "\n",
    "    word_counts = Counter()\n",
    "    for word, count in words_and_counts:\n",
    "        word_counts[word] += count\n",
    "\n",
    "    word, count = word_counts.most_common(1)[0]\n",
    "\n",
    "    yield (user, (word, count))\n",
    "\n",
    "user_words = map_reduce(status_updates,\n",
    "                        words_per_user_mapper,\n",
    "                        most_popular_word_reducer)\n",
    "print(user_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## YOUR TASK 7\n",
    "\n",
    "1. Adjust the above most_popular_word_reducer() function so that you only show users who have used a word more than once.\n",
    "2. How do you need to change above's code to get a general 'Top-k' counter which allows to determine the *k* most common words per user?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('joelgrus', [('data', 2), ('is', 2), ('interested', 1), ('anyone', 1)]),\n",
       " ('ben', [('science', 3), ('data', 3), ('fun', 2), ('is', 1)]),\n",
       " ('uwe', [('fun', 1), ('are', 1), ('databases', 1)])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "def most_popular_word_nosingles_reducer(user, words_and_counts):\n",
    "    \"\"\"given a sequence of (word, count) pairs,\n",
    "    return the word with the highest total count\"\"\"\n",
    "\n",
    "    word_counts = Counter()\n",
    "    for word, count in words_and_counts:\n",
    "        word_counts[word] += count\n",
    "\n",
    "    word, count = word_counts.most_common(1)[0]\n",
    "\n",
    "    if count > 1:\n",
    "        yield (user, (word, count))\n",
    "\n",
    "map_reduce(status_updates,\n",
    "                        words_per_user_mapper,\n",
    "                        most_popular_word_nosingles_reducer)\n",
    "\n",
    "# your code here\n",
    "def most_popular_k_words_reducer(k, user, words_and_counts):\n",
    "    \"\"\"given a sequence of (word, count) pairs,\n",
    "    return the word with the highest total count\"\"\"\n",
    "    word_counts = Counter()\n",
    "    for word, count in words_and_counts:\n",
    "        word_counts[word] += count\n",
    "\n",
    "    top_k = word_counts.most_common(k)\n",
    "    yield (user, top_k)\n",
    "\n",
    "map_reduce(status_updates,\n",
    "                        words_per_user_mapper,\n",
    "                        partial(most_popular_k_words_reducer, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
