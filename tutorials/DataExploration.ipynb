{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration with Python\n",
    "\n",
    "## EXERCISE: Reading and accessing data\n",
    "\n",
    "### Read the survey response data\n",
    "\n",
    "The `csv` module supports reading and writing of files in comma-separated values (CSV) and similar formats. We use `DictReader` since the first row of our survey responses file is a header. This produces a list of dictionaries, one dictionary per 57 responses. The `pprint` command below prints the dictionary corresponding the the first response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '02_ds_survey_responses_raw_20160301.csv'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d4511c476851>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'02_ds_survey_responses_raw_20160301.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '02_ds_survey_responses_raw_20160301.csv'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import csv\n",
    "import pprint\n",
    "data = list(csv.DictReader(open('02_ds_survey_responses_raw_20160301.csv')))\n",
    "pprint.pprint(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's define constants for dictionary keys\n",
    "\n",
    "Before moving on, let's define constants for the keys of this dictionary that will make it a bit easier to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = 'Timestamp'\n",
    "BACKGROUND_INDUSTRY = 'What main industry have you worked in?'\n",
    "BACKGROUND_YEARS_PROFESSIONAL = 'How many years professional experience do you have?'\n",
    "BACKGROUND_YEARS_PROGRAMMING = 'How many years programming experience do you have?'\n",
    "BACKGROUND_SKILLS = 'What key experience do you have?'\n",
    "IMPORT_DATA_MANAGEMENT = 'Data management'\n",
    "IMPORT_STATISTICS = 'Statistics'\n",
    "IMPORT_VISUALISATION = 'Visualisation'\n",
    "IMPORT_MACHINE_LEARNING = 'Machine learning'\n",
    "IMPORT_SOFTWARE_ENGINEERING = 'Software engineering'\n",
    "IMPORT_COMMUNICATION = 'Communication'\n",
    "GOALS_DEFINITION = 'How would you define data science in one sentence?'\n",
    "GOALS_SKILLS = 'What key skills do you want to learn?'\n",
    "GOALS_ROLE = 'What kind of role would you like to go into?'\n",
    "GOALS_INDUSTRY = 'What industry would you like to go into?'\n",
    "IMPORT_AREAS = [\n",
    "    IMPORT_DATA_MANAGEMENT,\n",
    "    IMPORT_STATISTICS,\n",
    "    IMPORT_VISUALISATION,\n",
    "    IMPORT_MACHINE_LEARNING,\n",
    "    IMPORT_SOFTWARE_ENGINEERING,\n",
    "    IMPORT_COMMUNICATION\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing data values\n",
    "\n",
    "This allows us to access cells in a row using the column name as a key. For example, the following prints the number of years professional experience for the first respondent. Note that the csv module reads all values as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = data[0] # Row 0 corresponds to first respondent since arrays are 0-indexed\n",
    "print(\"response:\", row[BACKGROUND_YEARS_PROFESSIONAL]) # years of professional experience\n",
    "print(\"type:\", type(row[BACKGROUND_YEARS_PROFESSIONAL])) # csv \n",
    "print(\"type:\", type(float(row[BACKGROUND_YEARS_PROFESSIONAL]))) # convert to float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO What is the third respondent's rating for communication?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[2][IMPORT_COMMUNICATION]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *STOP PLEASE. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Frequency distribution and mode\n",
    "\n",
    "### Counting data\n",
    "\n",
    "`Counter` from the `collections` module is useful for quickly calculating frequencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter([row[IMPORT_COMMUNICATION] for row in data])\n",
    "print(\"Distribution of communication importance ratings:\")\n",
    "for k, v in sorted(c.items()):\n",
    "    print('{}: {}'.format(k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Calculate distribution of background and goal industries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_distr(data, column_key):\n",
    "    print(column_key.upper())\n",
    "    c = Counter([row[column_key] for row in data])\n",
    "    for k,v in sorted(c.items()):\n",
    "        print('* {}: {}'.format(k, v))\n",
    "print_distr(data, BACKGROUND_INDUSTRY)\n",
    "print_distr(data, GOALS_INDUSTRY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the mode\n",
    "\n",
    "We can also use `Counter` to calcualte the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode(data, column_key):\n",
    "    c = Counter([row[column_key] for row in data])\n",
    "    return c.most_common(1)[0][0]\n",
    "print(\"Communication mode:\", mode(data, IMPORT_COMMUNICATION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Calculate the mode of background and goal industries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Background industry mode:\", mode(data, BACKGROUND_INDUSTRY))\n",
    "print(\"Goals industry mode:\", mode(data, GOALS_INDUSTRY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *STOP PLEASE. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Calculating descriptive statistics\n",
    "\n",
    "### Cleaning float data\n",
    "\n",
    "Which columns contained ratio data? We need to convert these to numeric types. Let's define a function since we have two ratio variables. Here we replace values that can't be converted with NaN (not a number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "DEFAULT_VALUE = np.nan\n",
    "def iter_clean(data, column_key, convert_function, default_value):\n",
    "    for row in data:\n",
    "        old_value = row[column_key]\n",
    "        new_value = default_value\n",
    "        try:\n",
    "            new_value = convert_function(old_value)\n",
    "        except (ValueError, TypeError):\n",
    "            warnings.warn('Replacing {} with {} in column {}'.format(\n",
    "                row[column_key], new_value, column_key))\n",
    "        row[column_key] = new_value\n",
    "        yield row\n",
    "data = list(iter_clean(data, BACKGROUND_YEARS_PROFESSIONAL, float, DEFAULT_VALUE))\n",
    "data = list(iter_clean(data, BACKGROUND_YEARS_PROGRAMMING, float, DEFAULT_VALUE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning timestamp data\n",
    "\n",
    "We may also want to conver timestamp values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "FMT = \"%m/%d/%Y %H:%M:%S\"\n",
    "def str_to_time(s):\n",
    "    return datetime.strptime(s, FMT)\n",
    "data = list(iter_clean(data, TIMESTAMP, str_to_time, DEFAULT_VALUE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics with `numpy`\n",
    "\n",
    "Once the data is converted, we can calculate descriptive statistics. `numpy` includes routines for measures of centrality and dispersion. Below we calculate descriptive statistics for professional and programming experience.\n",
    "\n",
    "Further detail: http://docs.scipy.org/doc/numpy/reference/routines.statistics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for column_key in [BACKGROUND_YEARS_PROFESSIONAL, BACKGROUND_YEARS_PROGRAMMING]:\n",
    "    v = [row[column_key] for row in data] # grab values\n",
    "    print(column_key.upper())\n",
    "    print(\"* Min..Max: {}..{}\".format(np.nanmin(v), np.nanmax(v)))\n",
    "    print(\"* Range: {}\".format(np.nanmax(v)-np.nanmin(v)))\n",
    "    print(\"* Mean: {}\".format(np.nanmean(v)))\n",
    "    print(\"* Standard deviation: {}\".format(np.nanstd(v)))\n",
    "    print(\"* Median: {}\".format(np.nanmedian(v)))\n",
    "    q1 = np.nanpercentile(v, 25)\n",
    "    print(\"* 25th percentile (Q1): {}\".format(q1))\n",
    "    q3 = np.nanpercentile(v, 75)\n",
    "    print(\"* 75th percentile (Q3): {}\".format(q3))\n",
    "    iqr = q3-q1\n",
    "    print(\"* IQR: {}\".format(iqr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning and histograms\n",
    "\n",
    "`numpy` also provides routines for binning and producing histograms from ratio data.\n",
    "\n",
    "NOTE RuntimeWarning due to NaN values, which are then ignored in histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [row[BACKGROUND_YEARS_PROFESSIONAL] for row in data] # grab values\n",
    "freqs, bins = np.histogram(v, bins=7, range=(0,35)) # calculate frequencies and bin start/end\n",
    "for i, freq in enumerate(freqs):\n",
    "    # Note that bins[i] <= bin_values < bins[i+1]\n",
    "    bin_str = '[{}..{})'.format(int(bins[i]), int(bins[i+1]))\n",
    "    print(bin_str, ':', freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Calculate histogram for programming experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [row[BACKGROUND_YEARS_PROGRAMMING] for row in data] # grab values\n",
    "freqs, bins = np.histogram(v, bins=7, range=(0,35)) # calculate frequencies and bin start/end\n",
    "for i, freq in enumerate(freqs):\n",
    "    # Note that bins[i] <= bin_values < bins[i+1]\n",
    "    bin_str = '[{}..{})'.format(int(bins[i]), int(bins[i+1]))\n",
    "    print(bin_str, ':', freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRA Calculate histograms with bin size of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_histogram(data, column_key, bin_size, minimum, maximum):\n",
    "    v = [row[column_key] for row in data] # grab values\n",
    "    num_bins = (maximum-minimum)/bin_size\n",
    "    return np.histogram(v, bins=num_bins, range=(minimum,maximum))\n",
    "\n",
    "def print_histogram(freqs, bins, column_key):\n",
    "    print(column_key.upper())\n",
    "    for i, freq in enumerate(freqs):\n",
    "        bin_str = '[{}..{})'.format(int(bins[i]), int(bins[i+1]))\n",
    "        print(bin_str, ':', freq)\n",
    "        \n",
    "prof_freqs, prof_bins = get_histogram(data, BACKGROUND_YEARS_PROFESSIONAL, 2, 0, 30)\n",
    "print_histogram(prof_freqs, prof_bins, BACKGROUND_YEARS_PROFESSIONAL)\n",
    "prog_freqs, prog_bins = get_histogram(data, BACKGROUND_YEARS_PROGRAMMING, 2, 0, 30)\n",
    "print_histogram(prog_freqs, prog_bins, BACKGROUND_YEARS_PROGRAMMING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *STOP PLEASE. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Visualisation with matplotlib\n",
    "\n",
    "### Making a frequency polygon\n",
    "\n",
    "`matplotlib` provides functionality for creating various plots. Let's start with a frequency polygon. Note the line `%matplotlib inline` in is important in Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "x_values = range(len(data))\n",
    "professional_experience = [row[BACKGROUND_YEARS_PROFESSIONAL] for row in data]\n",
    "plt.plot(x_values, professional_experience, 'g-', label='Professional')\n",
    "plt.title('Experience')\n",
    "plt.ylabel('Number of responses')\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Add programming experience to the plot\n",
    "\n",
    "Hint: Copy code above and add extra plt.plot command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "x_values = range(len(data))\n",
    "professional_experience = [row[BACKGROUND_YEARS_PROFESSIONAL] for row in data]\n",
    "plt.plot(x_values, professional_experience, 'g-', label='Professional')\n",
    "programming_experience = [row[BACKGROUND_YEARS_PROGRAMMING] for row in data]\n",
    "plt.plot(x_values, programming_experience, 'r-', label='Programming')\n",
    "plt.title('Experience')\n",
    "plt.ylabel('Number of responses')\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A real frequency polygon\n",
    "\n",
    "The above illustrate how to build a line chart in matplotlib but is not really frequency polygon as the x-axis corresponds to individual answers rather than groups. A more useful frequency poloygon uses the histogram data from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "x_values = list(range(len(prof_freqs)))\n",
    "x_labels = ['[{}..{})'.format(int(prof_bins[i]), int(prof_bins[i+1])) for i in range(len(prof_freqs))]\n",
    "plt.plot(x_values, prof_freqs, 'g-', label='Professional')\n",
    "plt.plot(x_values, prog_freqs, 'r-', label='Programming')\n",
    "plt.xticks(x_values, x_labels, rotation='vertical')\n",
    "plt.title('Experience')\n",
    "plt.ylabel('Number of responses')\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a bar chart\n",
    "\n",
    "Now let's make a bar chart of communication importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "IMPORT_KEYS = ['1', '2', '3', '4', '5']\n",
    "def make_importance_plot(data, column_key, title):\n",
    "    c = Counter(row[column_key] for row in data)\n",
    "    d = OrderedDict([(k,c[k]) if k in c else (k,0) for k in IMPORT_KEYS])\n",
    "    # bars are by default width 0.8, so we'll add 0.1 to the left coordinates\n",
    "    xs = [i+0.1 for i,_ in enumerate(IMPORT_KEYS)]\n",
    "    plt.bar(xs, d.values())\n",
    "    plt.ylabel('Number of responses')\n",
    "    plt.axis([0,5,0,35])\n",
    "    plt.title(title)\n",
    "    plt.xticks([i + 0.5 for i, _ in enumerate(IMPORT_KEYS)], IMPORT_KEYS)\n",
    "    plt.show()\n",
    "for a in IMPORT_AREAS:\n",
    "    title = 'Importance of {}'.format(a.lower())\n",
    "    make_importance_plot(data, a, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Make bar charts of known and future industries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def iter_values(data, column_key):\n",
    "    for row in data:\n",
    "        yield(row[column_key])\n",
    "\n",
    "background_industries = set(iter_values(data, BACKGROUND_INDUSTRY))\n",
    "goals_industries = set(iter_values(data, GOALS_INDUSTRY))\n",
    "all_industries = sorted(background_industries.union(goals_industries))\n",
    "\n",
    "def make_bar_chart(data, column_key, values, y_min, y_max):\n",
    "    c = Counter(row[column_key] for row in data)\n",
    "    d = OrderedDict([(k,c[k]) if k in c else (k,0) for k in values])\n",
    "    # bars are by default width 0.8, so we'll add 0.1 to the left coordinates\n",
    "    xs = [i+0.1 for i,_ in enumerate(values)]\n",
    "    plt.bar(xs, d.values())\n",
    "    plt.ylabel('Number of responses')\n",
    "    plt.axis([0,len(values),y_min,y_max])\n",
    "    plt.title(column_key)\n",
    "    plt.xticks([i + 0.5 for i, _ in enumerate(values)], values, rotation='vertical')\n",
    "    plt.show()\n",
    "\n",
    "make_bar_chart(data, BACKGROUND_INDUSTRY, all_industries, 0, 25)\n",
    "make_bar_chart(data, GOALS_INDUSTRY, all_industries, 0, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a histogram\n",
    "\n",
    "Now let's use the `histogram` from `numpy` above to create a histograms of professional and programming experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_histogram(data, column_key):\n",
    "    v = [row[column_key] for row in data] # grab values\n",
    "    freqs, bins = np.histogram(v, bins=7, range=(0,35))\n",
    "    for i, freq in enumerate(freqs):\n",
    "        yield ('[{}..{})'.format(int(bins[i]), int(bins[i+1])), freq)\n",
    "        \n",
    "def make_histogram_plot(data, column_key, title):\n",
    "    d = OrderedDict(iter_histogram(data, column_key))\n",
    "    keys = list(d.keys())\n",
    "    xs = [i+0.1 for i,_ in enumerate(keys)]\n",
    "    plt.bar(xs, d.values())\n",
    "    plt.ylabel('Number of responses')\n",
    "    plt.axis([0,7,0,35])\n",
    "    plt.title(title)\n",
    "    plt.xticks([i + 0.5 for i, _ in enumerate(keys)], keys)\n",
    "    plt.show()\n",
    "    \n",
    "make_histogram_plot(data, BACKGROUND_YEARS_PROFESSIONAL, 'Professional experience')\n",
    "make_histogram_plot(data, BACKGROUND_YEARS_PROGRAMMING, 'Programming experience')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a scatterplot\n",
    "\n",
    "Finally, let's make a scatterplot to compare professional and programming experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "professional_experience = [row[BACKGROUND_YEARS_PROFESSIONAL] for row in data]\n",
    "programming_experience = [row[BACKGROUND_YEARS_PROGRAMMING] for row in data]\n",
    "plt.scatter(professional_experience, programming_experience)\n",
    "plt.title('Professional vs programming experience')\n",
    "plt.xlabel('Years of professional experience')\n",
    "plt.ylabel('Years of programming experience')\n",
    "plt.axis([-1,35,-1,35])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *STOP PLEASE. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Box plots and correlation\n",
    "\n",
    "### Visualising distributions with box plots\n",
    "\n",
    "Mean and standard deviation are not informative for skewed data. `boxplot` is is a good visualisation for viewing and comparing distributions. It also shows outliers, e.g., values greater than `Q3+1.5*IQR` or less than `Q1-1.5*IQR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "professional_experience = [v for v in professional_experience if v is not np.nan]\n",
    "programming_experience = [v for v in programming_experience if v is not np.nan]\n",
    "plt.boxplot([professional_experience, programming_experience], \n",
    "            vert=False, notch=True, flierprops={'marker':'.'})\n",
    "plt.axis([-1,35,0,3])\n",
    "plt.yticks([1,2], ['Professional', 'Programming'])\n",
    "plt.xlabel('Years experience')\n",
    "plt.title('Professional vs programming experience')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating correlation between two variables\n",
    "\n",
    "Pearson's r is the covariance of the two variables divided by the product of their standard deviations. Spearman rho is a common nonparametric test that is used in stead of Pearson's r when "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "# only keep rows where both professional and programming experience are defined\n",
    "prof, prog = [], []\n",
    "for row in data:\n",
    "    if row[BACKGROUND_YEARS_PROFESSIONAL] is np.nan:\n",
    "        continue # ignore rows with no value for professional experience\n",
    "    elif row[BACKGROUND_YEARS_PROGRAMMING] is np.nan:\n",
    "        continue # ignore rows with no value for programming experience\n",
    "    else:\n",
    "        prof.append(row[BACKGROUND_YEARS_PROFESSIONAL])\n",
    "        prog.append(row[BACKGROUND_YEARS_PROGRAMMING])\n",
    "print(\"Pearson (r, p): {}\".format(stats.pearsonr(prof, prog)))\n",
    "\n",
    "print(stats.spearmanr(prof, prog))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Calculate Kendall's tau between importance ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:.2f}'.format(3.14159))\n",
    "for i,k1 in enumerate(IMPORT_AREAS):\n",
    "    print(k1)\n",
    "    v1 = [r[k1] for r in data]\n",
    "    for j in range(i+1,len(IMPORT_AREAS),1):\n",
    "        k2 = IMPORT_AREAS[j]\n",
    "        v2 = [r[k2] for r in data]\n",
    "        tau = stats.kendalltau(v1, v2)\n",
    "        print('* {:5.2f} (p={:.3f}): {}'.format(tau.correlation, tau.pvalue, k2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *STOP PLEASE. THE FOLLOWING IS FOR THE NEXT EXERCISE. THANKS.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Text data\n",
    "\n",
    "### Simple tokenisation and word counts\n",
    "\n",
    "Tokenisation is the process of breaking text into it's component parts, e.g., sentences, words. Below is a simple whitespace tokeniser that also removes some leading/trailing punctuation. We can use this to count the frequency of terms acros our data science definitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(text):\n",
    "    for word in text.lower().split():\n",
    "        yield word.strip('.,')\n",
    "\n",
    "def is_valid_word(w):\n",
    "    if w == '':\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def iter_ds_def_words(d):\n",
    "    for row in d:\n",
    "        for word in tokenise(row[GOALS_DEFINITION]):\n",
    "            if is_valid_word(word):\n",
    "                yield word\n",
    "\n",
    "from collections import Counter\n",
    "c = Counter(iter_ds_def_words(data))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop words\n",
    "\n",
    "Very common function words can be removed to focus our analysis on content words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = frozenset([ # http://www.nltk.org/book/ch02.html#stopwords_index_term\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "    'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "    'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "    'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "    'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "    'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "    'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "    'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "    'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "    'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "    'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'\n",
    "    ])\n",
    "def is_valid_word(w):\n",
    "    if w == '':\n",
    "        return False\n",
    "    if w.lower() in STOP_WORDS:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "c = Counter(iter_ds_def_words(data))\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting term frequencies\n",
    "\n",
    "Now we can build a simple horizontal bar chart that displays the most common terms across data science definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def iter_word_freqs(d, min_freq=4):\n",
    "    c = Counter(iter_ds_def_words(d))\n",
    "    for term, freq in c.items():\n",
    "        if freq >= min_freq:\n",
    "            yield term, freq\n",
    "\n",
    "d = OrderedDict([(k,v) for k,v in sorted(iter_word_freqs(data), key=operator.itemgetter(1))])\n",
    "ys = [i+0.5 for i,_ in enumerate(d)]\n",
    "plt.barh(ys, d.values(), align='center')\n",
    "plt.yticks(ys, list(d.keys()))\n",
    "plt.axis([0,50,0-0.1,len(d)+0.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
